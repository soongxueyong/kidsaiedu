{
  "captionData": [
    {
      "lang": "zh-Hans", 
      "captions": [
        {
          "dur": 3.079, 
          "text": "&gt;&gt; ELIZABETH KEMP\uff1a\n\u8fd9\u662f\u4e00\u4e2aMLCC\u795e\u7ecf\u7f51\u7edc\u87ba\u65cb\u7ec3\u4e60\u3002", 
          "start": 0.55
        }, 
        {
          "dur": 6.72, 
          "text": "\u5982\u679c\u4f7f\u7528\u9ed8\u8ba4\u53c2\u6570\uff0c\n\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6a21\u578b\u7684\u8868\u73b0", 
          "start": 3.629
        }, 
        {
          "dur": 2.811, 
          "text": "\u975e\u5e38\u7cdf\u7cd5\u3002", 
          "start": 10.349
        }, 
        {
          "dur": 7.679, 
          "text": "\u5373\u4f7f\u5df2\u7ecf\u8fc72500\u6b21\u8fed\u4ee3\uff0c\n\u6d4b\u8bd5\u635f\u5931\u4ecd\u9ad8\u4e8e50%\uff0c", 
          "start": 13.16
        }, 
        {
          "dur": 2.84, 
          "text": "\u6a21\u578b\u6839\u672c\u5c31\u6ca1\u6709\u5b66\u4e60\u6570\u636e\u3002", 
          "start": 20.839
        }, 
        {
          "dur": 3.821, 
          "text": "\u8ba9\u6211\u4eec\u6765\u770b\u770b\u80fd\u4e0d\u80fd\u5bf9\u5176\u8fdb\u884c\u6539\u8fdb\u3002", 
          "start": 23.679
        }, 
        {
          "dur": 5.168, 
          "text": "\u5728\u8fd9\u79cd\u76d8\u65cb\u4ea4\u7ec7\u7684\u87ba\u65cb\u4e2d\uff0c\n\u867d\u7136\u6570\u636e\u770b\u8d77\u6765\u5f88\u590d\u6742\uff0c", 
          "start": 27.5
        }, 
        {
          "dur": 4.381, 
          "text": "\u4f46\u5982\u679c\u4f7f\u7528\u5408\u9002\u7684\u7279\u5f81\u548c\u53c2\u6570\uff0c\n\u5e76\u4e0d\u9700\u8981\u8d85\u6df1\u6216\u8d85\u590d\u6742\u7684\u6a21\u578b", 
          "start": 32.668
        }, 
        {
          "dur": 1.0, 
          "text": "\u4fbf\u53ef\u89e3\u51b3\u95ee\u9898\u3002", 
          "start": 37.049
        }, 
        {
          "dur": 4.61, 
          "text": "\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u5728\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u65f6", 
          "start": 38.049
        }, 
        {
          "dur": 2.579, 
          "text": "\u9700\u8981\u6ce8\u610f\u7684\u51e0\u70b9\u3002", 
          "start": 42.659
        }, 
        {
          "dur": 6.94, 
          "text": "\u7b2c\u4e00\u9879\u4efb\u52a1\u662f\n\u4ec5\u4f7f\u7528X1\u548cX2\u6765\u8bad\u7ec3\u6a21\u578b\uff0c", 
          "start": 45.238
        }, 
        {
          "dur": 1.77, 
          "text": "\u800c\u4e0d\u8fdb\u884c\u4efb\u4f55\u5176\u4ed6\u7279\u5f81\u5de5\u7a0b\u3002", 
          "start": 52.179
        }, 
        {
          "dur": 8.5, 
          "text": "\u6211\u8981\u5c1d\u8bd5\u7684\u7b2c\u4e00\u4e2a\u6a21\u578b\u662f\u6dfb\u52a0\u5f88\u591a\u5c42\uff0c\n\u5e76\u5728\u6bcf\u5c42\u4e2d\u6dfb\u52a0\u5f88\u591a\u795e\u7ecf\u5143\uff0c", 
          "start": 53.948
        }, 
        {
          "dur": 6.211, 
          "text": "\u56e0\u4e3a\u8d8a\u591a\u8d8a\u597d\uff0c\u5bf9\u5427\uff1f", 
          "start": 62.448
        }, 
        {
          "dur": 8.41, 
          "text": "[\u6682\u505c]\u4e8e\u662f\u7b2c\u4e00\u4e2a\u95ee\u9898\u51fa\u73b0\u4e86\uff0c\n\u5c31\u662f\u56fe\u4e2d\u7684\u8282\u70b9\u592a\u591a\u3002", 
          "start": 68.659
        }, 
        {
          "dur": 4.61, 
          "text": "\u8be5\u6a21\u578b\u7684\u8bad\u7ec3\u901f\u5ea6\u975e\u5e38\u6162\uff0c\u56e0\u4e3a\u5982\u679c\n\u8282\u70b9\u5f88\u591a\u7684\u8bdd\uff0c\u8981\u8c03\u6574\u56fe\u4e2d\u7684\u6240\u6709\u6743\u91cd\uff0c", 
          "start": 77.069
        }, 
        {
          "dur": 1.83, 
          "text": "\u9700\u8981\u8fdb\u884c\u5927\u91cf\u7684\u8ba1\u7b97\u3002", 
          "start": 81.68
        }, 
        {
          "dur": 3.599, 
          "text": "\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\n\u8fed\u4ee3\u6b21\u6570\u7684\u53d8\u5316\u975e\u5e38\u6162\uff0c", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "\u800c\u4e14\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e5f\u4e0d\u662f\u5f88\u597d\uff0c\n\u56e0\u4e3a\u56fe\u4e2d\u7684\u5c42\u548c\u8282\u70b9\u592a\u591a\u4e86\u3002", 
          "start": 87.109
        }, 
        {
          "dur": 4.44, 
          "text": "\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u8fd8\u6709\u4e9b\u610f\u4e49\uff0c", 
          "start": 92.719
        }, 
        {
          "dur": 3.39, 
          "text": "\u4ee3\u8868\u8fd9\u4e9b\u4e0d\u540c\u89d2\u5ea6\u7684\u7ebf\u6027\u7279\u5f81\u3002", 
          "start": 97.159
        }, 
        {
          "dur": 4.541, 
          "text": "\u4f46\u968f\u7740\u5411\u56fe\u4e2d\u95f4\u9760\u8fd1\uff0c\n\u5c31\u53d8\u5f97\u5f88\u96be\u6e05\u6670\u5224\u65ad", 
          "start": 100.549
        }, 
        {
          "dur": 6.239, 
          "text": "\u67d0\u4e9b\u7279\u5f81\u5bf9\u87ba\u65cb\u6570\u636e\u7ed3\u679c\u7684\u8d21\u732e\u6709\u591a\u5c11\u3002", 
          "start": 105.09
        }, 
        {
          "dur": 6.71, 
          "text": "[\u6682\u505c]\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u6d4b\u8bd5\u635f\u5931\u5df2\u6709\u6240\u6539\u5584\u3002", 
          "start": 111.329
        }, 
        {
          "dur": 6.83, 
          "text": "\u73b0\u5728\u5df2\u7ecf\u4f4e\u4e8e50%\u4e86\uff0c\n\u4f46\u66f2\u7ebf\u8fd8\u4e0d\u662f\u5f88\u5e73\u6ed1\u3002", 
          "start": 118.04
        }, 
        {
          "dur": 4.87, 
          "text": "\u8ba9\u6211\u4eec\u6765\u770b\u770b\u662f\u4e0d\u662f\u53ef\u4ee5\u51cf\u5c11\u8282\u70b9\u6570\uff0c\n\u80fd\u4e0d\u80fd\u83b7\u5f97\u66f4\u6709\u610f\u4e49\u4e14\u901f\u5ea6\u66f4\u5feb\u7684\u6a21\u578b\uff0c", 
          "start": 124.87
        }, 
        {
          "dur": 7.9, 
          "text": "\u4ee5\u53ca\u80fd\u4e0d\u80fd\u901a\u8fc7\u8c03\u6574\u53c2\u6570\n\u6765\u5b9e\u73b0\u66f4\u7406\u60f3\u7684\u6d4b\u8bd5\u635f\u5931\u3002", 
          "start": 129.74
        }, 
        {
          "dur": 5.56, 
          "text": "\u73b0\u5728\u56fe\u4e2d\u67094\u4e2a\u9690\u85cf\u5c42\uff0c\n\u517120\u4e2a\u8282\u70b9\uff0c\u8282\u70b9\u6570\u4e0d\u5230\u4e4b\u524d\u7684\u4e00\u534a\uff0c", 
          "start": 137.639
        }, 
        {
          "dur": 2.66, 
          "text": "\u4e4b\u524d\u662f48\u4e2a\u3002", 
          "start": 143.199
        }, 
        {
          "dur": 2.42, 
          "text": "\u867d\u7136\u4ecd\u4e0d\u662f\u4e00\u4e2a\u8d85\u7ea7\u7b80\u5355\u7684\u7f51\u7edc\uff0c\n\u4f46\u5df2\u7ecf\u6709\u6240\u6539\u5584\uff0c", 
          "start": 145.86
        }, 
        {
          "dur": 2.28, 
          "text": "\u8bad\u7ec3\u901f\u5ea6\u4e5f\u53d8\u5feb\u4e86\u3002", 
          "start": 148.28
        }, 
        {
          "dur": 5.53, 
          "text": "\u63a5\u4e0b\u6765\u8fd8\u8981\u964d\u4f4e\u5b66\u4e60\u901f\u7387\uff0c", 
          "start": 150.56
        }, 
        {
          "dur": 2.03, 
          "text": "\u4ee5\u514d\u635f\u5931\u66f2\u7ebf\u8df3\u6765\u8df3\u53bb\u3002", 
          "start": 156.09
        }, 
        {
          "dur": 5.83, 
          "text": "\u5e0c\u671b\u8fd9\u6b21\u80fd\u83b7\u5f97\u66f4\u5e73\u6ed1\u7684\u635f\u5931\u66f2\u7ebf\uff0c\n\u5e76\u4e14\u4e0d\u4f1a\u4e0e\u6d4b\u8bd5\u6570\u636e\u8fc7\u62df\u5408\u3002", 
          "start": 158.12
        }, 
        {
          "dur": 4.31, 
          "text": "\u6211\u4eec\u6765\u770b\u770b\u8fd9\u6b21\u7684\u8868\u73b0\u5982\u4f55\u3002", 
          "start": 163.949
        }, 
        {
          "dur": 7.581, 
          "text": "[\u6682\u505c]\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\n\u901a\u8fc7\u51cf\u5c11\u8282\u70b9\u5e76\u964d\u4f4e\u5b66\u4e60\u901f\u7387\uff0c", 
          "start": 168.259
        }, 
        {
          "dur": 1.0, 
          "text": "\u6a21\u578b\u5b66\u4e60\u901f\u5ea6\u53d8\u5feb\u4e86\uff0c", 
          "start": 175.84
        }, 
        {
          "dur": 5.289, 
          "text": "\u8fed\u4ee3\u901f\u5ea6\u5feb\u4e86\u5f88\u591a\uff0c\n\u4e0d\u8fc7\u6d4b\u8bd5\u635f\u5931\u8fd8\u662f\u6ca1\u4ec0\u4e48\u53d8\u5316\u3002", 
          "start": 176.84
        }, 
        {
          "dur": 5.021, 
          "text": "\u4f46\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u635f\u5931\u66f2\u7ebf\u4ecd\u7136\u5728\u8df3\u52a8\uff0c\n\u800c\u4e14\u6a21\u578b\u8f93\u51fa\u4e5f\u5c16\u9510\u4e86\u5f88\u591a\uff0c", 
          "start": 182.129
        }, 
        {
          "dur": 3.729, 
          "text": "\u4e0d\u50cf\u5c1d\u8bd5\u8981\u62df\u5408\u7684\u87ba\u65cb\u90a3\u6837\u5e73\u6ed1\u3002", 
          "start": 187.15
        }, 
        {
          "dur": 4.771, 
          "text": "\u8fd9\u5c31\u4ea7\u751f\u4e86\u6211\u4eec\u8981\u8bf4\u7684\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\n\u5c31\u662f\u6ca1\u6709\u6b63\u5219\u5316\u3002", 
          "start": 190.879
        }, 
        {
          "dur": 4.009, 
          "text": "\u6b63\u5219\u5316\u80fd\u591f\u964d\u4f4e\u6a21\u578b\u7684\u590d\u6742\u5ea6\uff0c\n\u5e76\u6539\u5584\u8fc7\u62df\u5408\u60c5\u51b5\u3002", 
          "start": 195.65
        }, 
        {
          "dur": 5.47, 
          "text": "\u6211\u4eec\u53ef\u4ee5\u770b\u4e00\u4e0b\uff0c\u5982\u679c\u6dfb\u52a0\u4e86\u6b63\u5219\u5316\uff0c\n\u6700\u540e\u80fd\u4e0d\u80fd\u83b7\u5f97\u66f4\u5e73\u6ed1\u7684\u635f\u5931\u66f2\u7ebf\uff0c", 
          "start": 199.659
        }, 
        {
          "dur": 6.14, 
          "text": "\u4ee5\u53ca\u66f4\u52a0\u7406\u60f3\u4e14\u4e0d\u4f1a\u8bd5\u56fe\n\u4e0e\u8bad\u7ec3\u6570\u636e\u8fc7\u62df\u5408\u7684\u62df\u5408\u6a21\u578b\u3002", 
          "start": 205.129
        }, 
        {
          "dur": 7.41, 
          "text": "\u73b0\u5728\u6211\u4eec\u6765\u4e3a\u6a21\u578b\u6dfb\u52a0\u6b63\u5219\u5316\uff0c\n\u770b\u770b\u80fd\u4e0d\u80fd\u6539\u5584\u66f2\u7ebf\u53ca\u62df\u5408\u60c5\u51b5\u3002", 
          "start": 211.27
        }, 
        {
          "dur": 5.779, 
          "text": "\u6211\u4eec\u9700\u8981\u786e\u4fdd\u6b63\u5219\u5316\u4e0d\u4f1a\u8fc7\u9ad8\uff0c\n\u5426\u5219\u5b83\u5c06\u65e0\u6cd5\u5b66\u4e60", 
          "start": 218.68
        }, 
        {
          "dur": 2.241, 
          "text": "\u6a21\u578b\u4e2d\u7684\u4efb\u4f55\u590d\u6742\u5ea6\u3002", 
          "start": 224.459
        }, 
        {
          "dur": 9.98, 
          "text": "[\u6682\u505c]\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7\u6dfb\u52a0\u6b63\u5219\u5316\uff0c\n\u6211\u4eec\u83b7\u5f97\u4e86\u66f4\u5e73\u6ed1\u7684\u635f\u5931\u66f2\u7ebf\uff0c", 
          "start": 226.699
        }, 
        {
          "dur": 7.75, 
          "text": "\u800c\u4e14\u6a21\u578b\u8f93\u51fa\u7684\u5c16\u9510\u7a0b\u5ea6\u4e5f\u5927\u5927\u964d\u4f4e\u4e86\u3002", 
          "start": 236.68
        }, 
        {
          "dur": 2.93, 
          "text": "\u4f60\u53ef\u4ee5\u5728\u6a21\u578b\u4e2d\u7ec3\u4e60\u8c03\u6574\u5f88\u591a\u5176\u4ed6\u53c2\u6570\uff0c", 
          "start": 244.43
        }, 
        {
          "dur": 3.849, 
          "text": "\u6bd4\u5982\u66f4\u6539\u6279\u91cf\u5927\u5c0f\uff0c\u5982\u679c\u6279\u91cf\u5927\u5c0f\u592a\u5c0f\uff0c", 
          "start": 247.36
        }, 
        {
          "dur": 2.25, 
          "text": "\u6a21\u578b\u8bad\u7ec3\u5c06\u9700\u8981\u5f88\u957f\u65f6\u95f4\uff1b", 
          "start": 251.209
        }, 
        {
          "dur": 2.67, 
          "text": "\u5982\u679c\u592a\u5927\uff0c\u53ef\u80fd\u4f1a\u5f88\u96be\u6536\u655b\u3002", 
          "start": 253.459
        }, 
        {
          "dur": 5.21, 
          "text": "\u548c\u8c03\u6574\u5b66\u4e60\u901f\u7387\u65f6\u4e00\u6837\uff0c\u5982\u679c\n\u6279\u91cf\u5927\u5c0f\u592a\u5927\uff0c\u635f\u5931\u66f2\u7ebf\u53ef\u80fd\u4f1a\u5267\u70c8\u8df3\u52a8\uff0c", 
          "start": 256.129
        }, 
        {
          "dur": 1.0, 
          "text": "\u5e76\u4e14\u5f88\u96be\u6536\u655b\u3002", 
          "start": 261.339
        }, 
        {
          "dur": 4.859, 
          "text": "\u4f60\u8fd8\u53ef\u4ee5\u7ec3\u4e60\u8c03\u6574\u6b63\u5219\u5316\u7387\u3002", 
          "start": 262.339
        }, 
        {
          "dur": 3.951, 
          "text": "\u5982\u679c\u8bad\u7ec3\u6570\u636e\u6ca1\u6709\u90a3\u4e48\u6742\u4e71\uff0c\n\u8be5\u6a21\u578b\u8fd8\u662f\u975e\u5e38\u51fa\u8272\u7684\u3002", 
          "start": 267.199
        }, 
        {
          "dur": 2.911, 
          "text": "\u73b0\u5728\u7684\u566a\u70b9\u9ad8\u8fbe80\uff0c", 
          "start": 271.149
        }, 
        {
          "dur": 5.81, 
          "text": "\u5982\u679c\u5c06\u566a\u70b9\u4e0b\u8c03\u81f330\u6216\u5176\u4ed6\u503c\uff0c\n\u7136\u540e\u5f00\u59cb\u8bad\u7ec3\u8be5\u6a21\u578b\uff0c", 
          "start": 274.06
        }, 
        {
          "dur": 2.06, 
          "text": "\u5c06\u4f1a\u5b9e\u73b0\u975e\u5e38\u7406\u60f3\u7684\u635f\u5931\u3002", 
          "start": 279.87
        }, 
        {
          "dur": 4.238, 
          "text": "\u4f46\u5982\u679c\u566a\u70b9\u975e\u5e38\u9ad8\uff0c\n\u6709\u65f6\u6dfb\u52a0\u4e00\u4e9b\u7279\u5f81\u5de5\u7a0b\uff0c", 
          "start": 281.93
        }, 
        {
          "dur": 5.71, 
          "text": "\u6bd4\u6dfb\u52a0\u5c42\u6570\u6216\u8c03\u6574\u53c2\u6570\n\u5b9e\u73b0\u7684\u6539\u8fdb\u6548\u679c\u8981\u597d\u5f97\u591a\u3002", 
          "start": 286.168
        }, 
        {
          "dur": 8.62, 
          "text": "[\u6682\u505c]\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\n\u964d\u4f4e\u566a\u70b9\u540e\uff0c\u6d4b\u8bd5\u635f\u5931\u6539\u5584\u4e86\u5f88\u591a\u3002", 
          "start": 291.879
        }, 
        {
          "dur": 3.711, 
          "text": "\u5373\u4f7f\u53ea\u4f7f\u7528\u7ebf\u6027\u7279\u5f81\uff0c\n\u4f46\u5982\u679c\u6570\u636e\u975e\u5e38\u6742\u4e71\uff0c", 
          "start": 300.499
        }, 
        {
          "dur": 6.049, 
          "text": "\u6211\u4eec\u4ecd\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u7279\u5f81\u5de5\u7a0b\uff0c\n\u624d\u80fd\u5f97\u5230\u66f4\u7406\u60f3\u7684\u6d4b\u8bd5\u635f\u5931\u3002", 
          "start": 304.209
        }, 
        {
          "dur": 2.49, 
          "text": "\u73b0\u5728\u6211\u4eec\u5c06\u566a\u70b9\u8c03\u56de\u523080%\uff0c", 
          "start": 310.259
        }, 
        {
          "dur": 2.361, 
          "text": "\u518d\u6dfb\u52a0\u4e00\u4e9b\u66f4\u590d\u6742\u7684\u7279\u5f81\uff0c", 
          "start": 312.749
        }, 
        {
          "dur": 6.358, 
          "text": "\u7136\u540e\u8bd5\u7740\u6dfb\u52a0\u6b63\u5f26\u51fd\u6570\uff0c\n\u4e5f\u53ef\u4ee5\u6dfb\u52a0X1\u7684\u5e73\u65b9\u548cX2\u7684\u5e73\u65b9\u3002", 
          "start": 315.11
        }, 
        {
          "dur": 4.332, 
          "text": "\u73b0\u5728\u770b\u4e00\u4e0b\u6a21\u578b\u7684\u8868\u73b0\u5982\u4f55\u3002", 
          "start": 321.468
        }, 
        {
          "dur": 7.928, 
          "text": "[\u6682\u505c]\u6dfb\u52a0\u66f4\u590d\u6742\u7684\u7279\u5f81\u540e\uff0c\n\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e9b\u4e25\u91cd\u7684\u95ee\u9898\uff0c", 
          "start": 325.8
        }, 
        {
          "dur": 4.512, 
          "text": "\u5176\u4e2d\u4e00\u4e2a\u5c31\u662f\u7279\u5f81\u7684\u590d\u6742\u5ea6\u5927\u5927\u6dfb\u52a0\u4e86\uff0c\n\u5c24\u5176\u662f\u5728\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u4e0a\u66f4\u662f\u5982\u6b64\u3002", 
          "start": 333.728
        }, 
        {
          "dur": 5.149, 
          "text": "\u795e\u7ecf\u5143\u6240\u5b66\u4e60\u7684\u5185\u5bb9\n\u8981\u6bd4\u659c\u7387\u4e0d\u540c\u7684\u7ebf", 
          "start": 338.24
        }, 
        {
          "dur": 2.88, 
          "text": "\u590d\u6742\u5f97\u591a\u3002", 
          "start": 343.389
        }, 
        {
          "dur": 7.471, 
          "text": "\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7\u6dfb\u52a0\u66f4\u590d\u6742\u7684\u7279\u5f81\uff0c\n\u6a21\u578b\u5b66\u4e60\u8fd9\u4e9b\u66f2\u7ebf\u7684\u901f\u5ea6", 
          "start": 346.269
        }, 
        {
          "dur": 1.41, 
          "text": "\u5feb\u4e86\u5f88\u591a\u3002", 
          "start": 353.74
        }, 
        {
          "dur": 4.759, 
          "text": "\u4f46\u4f60\u4ecd\u53ef\u4ee5\u770b\u5230\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u590d\u6742\u7684\u66f2\u7ebf\uff0c\n\u8fd9\u5c31\u53c8\u56de\u5230\u4e86\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c", 
          "start": 355.149
        }, 
        {
          "dur": 3.892, 
          "text": "\u5c31\u662f\u7f51\u7edc\u4e2d\u7684\u8282\u70b9\u592a\u591a\u4e86\u3002", 
          "start": 359.908
        }, 
        {
          "dur": 5.41, 
          "text": "\u73b0\u5728\u6211\u4eec\u8bd5\u7740\u53bb\u6389\u4e00\u4e9b\u5c42\u548c\u4e00\u4e9b\u795e\u7ecf\u5143\u3002", 
          "start": 363.8
        }, 
        {
          "dur": 6.509, 
          "text": "\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u901a\u8fc7\u51cf\u5c11\u5c42\u6570\n\u4ee5\u53ca\u5404\u5c42\u4e2d\u7684\u795e\u7ecf\u5143\u6570\uff0c", 
          "start": 369.209
        }, 
        {
          "dur": 5.911, 
          "text": "\u7136\u540e\u518d\u964d\u4f4e\u5b66\u4e60\u901f\u7387\uff0c\n\u53ef\u4ee5\u83b7\u5f97\u5e73\u6ed1\u5ea6\u5927\u5927\u63d0\u5347\u7684\u635f\u5931\u66f2\u7ebf\uff0c", 
          "start": 375.718
        }, 
        {
          "dur": 6.62, 
          "text": "\u5e76\u4e14\u6570\u636e\u62df\u5408\u60c5\u51b5\u4e5f\u6539\u5584\u4e86\u5f88\u591a\uff0c\n\u5373\u4f7f\u6570\u636e\u4ecd\u7136\u975e\u5e38\u6742\u4e71\uff0c", 
          "start": 381.629
        }, 
        {
          "dur": 5.73, 
          "text": "\u6211\u4eec\u6dfb\u52a0\u7684\u7279\u5f81\u53d1\u6325\u4e86\u5f88\u5927\u4f5c\u7528\uff0c\n\u901a\u8fc7\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c", 
          "start": 388.249
        }, 
        {
          "dur": 3.94, 
          "text": "\u5e76\u51cf\u5c11\u6a21\u578b\u4e2d\u7684\u5c42\u6570\uff0c\n\u53ef\u4ee5\u6781\u5927\u5730\u6539\u5584", 
          "start": 393.978
        }, 
        {
          "dur": 1.291, 
          "text": "\u6570\u636e\u8fc7\u62df\u5408\u7684\u60c5\u51b5\u3002", 
          "start": 397.918
        }, 
        {
          "dur": 5.699, 
          "text": "\u56e0\u6b64\u4f60\u53ef\u4ee5\u770b\u5230\uff0c\u5373\u4f7f\u662f\n\u53ea\u67091\u4e2a\u9690\u85cf\u5c42\u548c5\u4e2a\u795e\u7ecf\u5143\u7684\u8d85\u7b80\u5355\u6a21\u578b\uff0c", 
          "start": 399.209
        }, 
        {
          "dur": 5.431, 
          "text": "\u901a\u8fc7\u66f4\u6539\u4e00\u4e9b\u5176\u4ed6\u53c2\u6570\uff0c\n\u4f8b\u5982\u5b66\u4e60\u901f\u7387\u3001\u6b63\u5219\u5316\u7387\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c", 
          "start": 404.908
        }, 
        {
          "dur": 7.219, 
          "text": "\u83b7\u5f97\u7684\u6d4b\u8bd5\u635f\u5931\u4ee5\u53ca\u62df\u5408\u66f2\u7ebf\u7684\u5e73\u6ed1\u5ea6", 
          "start": 410.339
        }, 
        {
          "dur": 3.81, 
          "text": "\u751a\u81f3\u6bd4\u4e00\u4e9b\u66f4\u590d\u6742\u7684\u6a21\u578b\u8fd8\u8981\u597d\u5f88\u591a\uff0c", 
          "start": 417.559
        }, 
        {
          "dur": 1.8, 
          "text": "\u800c\u4e14\u66f2\u7ebf\u7684\u6536\u655b\u901f\u5ea6\u4e5f\u4f1a\u5feb\u5f88\u591a\u3002", 
          "start": 421.369
        }
      ]
    }, 
    {
      "lang": "en", 
      "captions": [
        {
          "dur": 3.079, 
          "text": "&gt;&gt; ELIZABETH KEMP: This is the MLCC neural\nnet spiral exercise.", 
          "start": 0.55
        }, 
        {
          "dur": 6.721, 
          "text": "So if we use the default parameters for the\nmodel, we can see that it really doesn&#39;t perform", 
          "start": 3.629
        }, 
        {
          "dur": 2.81, 
          "text": "well.", 
          "start": 10.35
        }, 
        {
          "dur": 7.68, 
          "text": "Even after 2,500 iterations, we can see that\nthe test loss is still above 50%, and the", 
          "start": 13.16
        }, 
        {
          "dur": 2.84, 
          "text": "model really isn&#39;t learning the data at all.", 
          "start": 20.84
        }, 
        {
          "dur": 3.82, 
          "text": "So let&#39;s see if we can do better.", 
          "start": 23.68
        }, 
        {
          "dur": 5.169, 
          "text": "Even though the data looks complex in this\nspiral--intertwined spirals, we don&#39;t need", 
          "start": 27.5
        }, 
        {
          "dur": 4.381, 
          "text": "a super deep or complex model to solve the\nproblem if we have the right features and", 
          "start": 32.669
        }, 
        {
          "dur": 1.0, 
          "text": "the right parameters.", 
          "start": 37.05
        }, 
        {
          "dur": 4.61, 
          "text": "I&#39;m going to talk through a couple of pitfalls\nto keep in mind when solving problems like", 
          "start": 38.05
        }, 
        {
          "dur": 2.579, 
          "text": "these.", 
          "start": 42.66
        }, 
        {
          "dur": 6.94, 
          "text": "The first task asks us to train the model\nusing only X1 and X2 without any additional", 
          "start": 45.239
        }, 
        {
          "dur": 1.77, 
          "text": "feature engineering.", 
          "start": 52.179
        }, 
        {
          "dur": 8.5, 
          "text": "So the first model I&#39;ll try is adding tons\nof layers and tons of neurons in each layer.", 
          "start": 53.949
        }, 
        {
          "dur": 6.211, 
          "text": "Because more is better, right?", 
          "start": 62.449
        }, 
        {
          "dur": 8.41, 
          "text": "[pause] And so this is our first pitfall,\ntoo many nodes in the graph.", 
          "start": 68.66
        }, 
        {
          "dur": 4.61, 
          "text": "This model is really slow because it&#39;s computationally\nexpensive to adjust all the weights in the", 
          "start": 77.07
        }, 
        {
          "dur": 1.83, 
          "text": "graph when you have so many nodes.", 
          "start": 81.68
        }, 
        {
          "dur": 3.6, 
          "text": "You can see it counting through the iterations\nreally slowly.", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "And also, the model is not very interpretable\nwith so many layers and nodes in the graph.", 
          "start": 87.11
        }, 
        {
          "dur": 4.44, 
          "text": "The first layer of the model sort of makes\nsense.", 
          "start": 92.72
        }, 
        {
          "dur": 3.39, 
          "text": "It&#39;s these linear features at different angles.", 
          "start": 97.16
        }, 
        {
          "dur": 4.54, 
          "text": "But as you get more into the middle of the\ngraph, some of these features are not really", 
          "start": 100.55
        }, 
        {
          "dur": 6.24, 
          "text": "clear how they contribute to the spiral data\noutcome.", 
          "start": 105.09
        }, 
        {
          "dur": 6.71, 
          "text": "[pause] So you can see our test loss has improved.", 
          "start": 111.33
        }, 
        {
          "dur": 6.83, 
          "text": "We&#39;re definitely below 50% now, but it&#39;s still\nnot a very smooth curve.", 
          "start": 118.04
        }, 
        {
          "dur": 4.87, 
          "text": "So let&#39;s see if we can reduce the number of\nnodes and see if we can get a faster model", 
          "start": 124.87
        }, 
        {
          "dur": 7.9, 
          "text": "that makes more sense and adjust the parameters\nas well to get better test loss.", 
          "start": 129.74
        }, 
        {
          "dur": 5.56, 
          "text": "So we now have four hidden layers in the graph\nwith 20 total nodes, which is less than half", 
          "start": 137.64
        }, 
        {
          "dur": 2.66, 
          "text": "of the 48 we had previously.", 
          "start": 143.2
        }, 
        {
          "dur": 2.42, 
          "text": "It&#39;s still not a super-simple network, but\nit&#39;s better.", 
          "start": 145.86
        }, 
        {
          "dur": 2.28, 
          "text": "It&#39;ll train faster.", 
          "start": 148.28
        }, 
        {
          "dur": 5.53, 
          "text": "And I&#39;m also going to reduce the learning\nrate so that we don&#39;t get all of that jumping", 
          "start": 150.56
        }, 
        {
          "dur": 2.03, 
          "text": "back and forth in the loss curve.", 
          "start": 156.09
        }, 
        {
          "dur": 5.83, 
          "text": "Hopefully this will give us a smoother loss\ncurve and not overfit to the test data.", 
          "start": 158.12
        }, 
        {
          "dur": 4.31, 
          "text": "So let&#39;s see how this performs.", 
          "start": 163.95
        }, 
        {
          "dur": 7.58, 
          "text": "[pause] With fewer nodes and a slower learning\nrate, you can see that the model is learning", 
          "start": 168.26
        }, 
        {
          "dur": 1.0, 
          "text": "faster.", 
          "start": 175.84
        }, 
        {
          "dur": 5.29, 
          "text": "The iterations are going much more quickly,\nbut we&#39;re still getting similar test loss.", 
          "start": 176.84
        }, 
        {
          "dur": 5.02, 
          "text": "But you can see that the loss curves are still\noscillating and that the model output is much", 
          "start": 182.13
        }, 
        {
          "dur": 3.73, 
          "text": "spikier, and not as smooth, as the spirals\nwe&#39;re trying to fit.", 
          "start": 187.15
        }, 
        {
          "dur": 4.77, 
          "text": "And that brings us to our next pitfall, which\nis no regularization.", 
          "start": 190.88
        }, 
        {
          "dur": 4.01, 
          "text": "So regularization penalizes complex models\nand overfitting.", 
          "start": 195.65
        }, 
        {
          "dur": 5.47, 
          "text": "So we can see if we add this--if we end up\ngetting a smoother loss curve and a better", 
          "start": 199.66
        }, 
        {
          "dur": 6.14, 
          "text": "fitting model that&#39;s not trying to overfit\nto the training data.", 
          "start": 205.13
        }, 
        {
          "dur": 7.41, 
          "text": "So let&#39;s add regularization to the model to\nsee if that improves the curve and the fit.", 
          "start": 211.27
        }, 
        {
          "dur": 5.779, 
          "text": "We want to make sure the regularization isn&#39;t\ntoo high or it won&#39;t learn any complexity", 
          "start": 218.68
        }, 
        {
          "dur": 2.241, 
          "text": "in the model.", 
          "start": 224.459
        }, 
        {
          "dur": 9.98, 
          "text": "[pause] So you can see that by adding regularization,\nwe&#39;re doing a better job of getting a smoother", 
          "start": 226.7
        }, 
        {
          "dur": 7.75, 
          "text": "loss curve, and it&#39;s also giving us a much\nless spiky model output.", 
          "start": 236.68
        }, 
        {
          "dur": 2.93, 
          "text": "So there are lots of other parameters that\nyou can play around with in the model.", 
          "start": 244.43
        }, 
        {
          "dur": 3.849, 
          "text": "You can change the batch size, and if it&#39;s\ntoo small, it&#39;s just gonna take a really long", 
          "start": 247.36
        }, 
        {
          "dur": 2.25, 
          "text": "time for your model to train.", 
          "start": 251.209
        }, 
        {
          "dur": 2.671, 
          "text": "If it&#39;s too large, it may have trouble converging.", 
          "start": 253.459
        }, 
        {
          "dur": 5.21, 
          "text": "Same with your learning rate, if it&#39;s too\nhigh it may jump around a lot and have trouble", 
          "start": 256.13
        }, 
        {
          "dur": 1.0, 
          "text": "converging.", 
          "start": 261.34
        }, 
        {
          "dur": 4.859, 
          "text": "You can play around with your regularization\nrate as well.", 
          "start": 262.34
        }, 
        {
          "dur": 3.951, 
          "text": "So this would actually be a pretty good model\nif the training data were less noisy.", 
          "start": 267.199
        }, 
        {
          "dur": 2.91, 
          "text": "We have the noise cranked up to 80 here.", 
          "start": 271.15
        }, 
        {
          "dur": 5.81, 
          "text": "So if we roll that down to 30, or something,\nand start training this model, this will actually", 
          "start": 274.06
        }, 
        {
          "dur": 2.06, 
          "text": "give us pretty good loss.", 
          "start": 279.87
        }, 
        {
          "dur": 4.239, 
          "text": "But with the high level of noise, at some\npoint adding a little bit of feature engineering", 
          "start": 281.93
        }, 
        {
          "dur": 5.71, 
          "text": "is going to give us a much bigger improvement\nthan more layers or tuning the parameters.", 
          "start": 286.169
        }, 
        {
          "dur": 8.62, 
          "text": "[pause] So you can see, by reducing the noise,\nwe can get a much better test loss.", 
          "start": 291.879
        }, 
        {
          "dur": 3.711, 
          "text": "Even--only using the linear features, but\nif we have that noisy data, we&#39;re going to", 
          "start": 300.499
        }, 
        {
          "dur": 6.049, 
          "text": "need to do some feature engineering to get\na better test loss.", 
          "start": 304.21
        }, 
        {
          "dur": 2.49, 
          "text": "So we have the noise back up at 80%.", 
          "start": 310.259
        }, 
        {
          "dur": 2.361, 
          "text": "Let&#39;s add some more complex features.", 
          "start": 312.749
        }, 
        {
          "dur": 6.359, 
          "text": "Let&#39;s try adding sin and maybe X1 squared\nand X2 squared.", 
          "start": 315.11
        }, 
        {
          "dur": 4.331, 
          "text": "Let&#39;s see how our model performs.", 
          "start": 321.469
        }, 
        {
          "dur": 7.929, 
          "text": "[pause] So one of the major things you notice\nby adding more complex features is that, especially", 
          "start": 325.8
        }, 
        {
          "dur": 4.511, 
          "text": "on the first layer of the model, the features\nare much more complex.", 
          "start": 333.729
        }, 
        {
          "dur": 5.149, 
          "text": "The things that the neurons are learning are\nmuch more complex than just lines at different", 
          "start": 338.24
        }, 
        {
          "dur": 2.881, 
          "text": "slopes.", 
          "start": 343.389
        }, 
        {
          "dur": 7.47, 
          "text": "So you can see that by adding the more complex\nfeatures, we are learning these curves at", 
          "start": 346.27
        }, 
        {
          "dur": 1.41, 
          "text": "a much faster rate.", 
          "start": 353.74
        }, 
        {
          "dur": 4.759, 
          "text": "But you can still see that this is a very\ncomplex curve, and that goes back to our first", 
          "start": 355.15
        }, 
        {
          "dur": 3.891, 
          "text": "pitfall, which is too many nodes in the network.", 
          "start": 359.909
        }, 
        {
          "dur": 5.41, 
          "text": "So let&#39;s try to get rid of some of these layers\nand some of these neurons.", 
          "start": 363.8
        }, 
        {
          "dur": 6.509, 
          "text": "So you can see that by reducing the number\nof layers and the number of neurons in each", 
          "start": 369.21
        }, 
        {
          "dur": 5.91, 
          "text": "layer, and I also turned down the learning\nrate, this is giving us a much smoother loss", 
          "start": 375.719
        }, 
        {
          "dur": 6.62, 
          "text": "curve and is fitting the data much better,\neven though we still have pretty noisy data.", 
          "start": 381.629
        }, 
        {
          "dur": 5.73, 
          "text": "So the features that we added have helped\na lot, and by reducing the model complexity,", 
          "start": 388.249
        }, 
        {
          "dur": 3.94, 
          "text": "the number of layers in the model, we are\ndoing a much better job of not overfitting", 
          "start": 393.979
        }, 
        {
          "dur": 1.291, 
          "text": "the data.", 
          "start": 397.919
        }, 
        {
          "dur": 5.699, 
          "text": "So you can see that even with a very, very\nsimple model of only one hidden layer and", 
          "start": 399.21
        }, 
        {
          "dur": 5.431, 
          "text": "five neurons, by changing a couple of the\nother parameters, like learning rate and regularization", 
          "start": 404.909
        }, 
        {
          "dur": 7.219, 
          "text": "rate and my activation function, I&#39;m getting\na much better test loss, a much smoother fitting", 
          "start": 410.34
        }, 
        {
          "dur": 3.81, 
          "text": "curve, than some of the more complex models.", 
          "start": 417.559
        }, 
        {
          "dur": 1.8, 
          "text": "And it&#39;s converging much more quickly.", 
          "start": 421.369
        }
      ]
    }, 
    {
      "lang": "fr", 
      "captions": [
        {
          "dur": 3.079, 
          "text": "Cet exercice porte sur un r\u00e9seau\nde neurones pour les donn\u00e9es en spirale.", 
          "start": 0.55
        }, 
        {
          "dur": 6.72, 
          "text": "Si nous utilisons les param\u00e8tres par d\u00e9faut\ndu mod\u00e8le, les performances ne sont pas", 
          "start": 3.629
        }, 
        {
          "dur": 2.811, 
          "text": "tr\u00e8s satisfaisantes.", 
          "start": 10.349
        }, 
        {
          "dur": 7.679, 
          "text": "M\u00eame apr\u00e8s 2\u00a0500\u00a0it\u00e9rations,\nla perte d&#39;\u00e9valuation reste sup\u00e9rieure \u00e0 50\u00a0%,", 
          "start": 13.16
        }, 
        {
          "dur": 2.84, 
          "text": "et le mod\u00e8le n&#39;apprend pas du tout les donn\u00e9es.", 
          "start": 20.839
        }, 
        {
          "dur": 3.821, 
          "text": "Voyons si nous pouvons faire mieux.", 
          "start": 23.679
        }, 
        {
          "dur": 5.168, 
          "text": "M\u00eame si les donn\u00e9es semblent complexes\nen forme de spirale entrelac\u00e9e, il est inutile", 
          "start": 27.5
        }, 
        {
          "dur": 4.381, 
          "text": "d&#39;avoir un mod\u00e8le tr\u00e8s profond ou complexe\nsi nous disposons des bonnes caract\u00e9ristiques", 
          "start": 32.668
        }, 
        {
          "dur": 1.0, 
          "text": "et des bons param\u00e8tres.", 
          "start": 37.049
        }, 
        {
          "dur": 4.61, 
          "text": "Voici quelques pi\u00e8ges \u00e0 \u00e9viter\nen r\u00e9solvant les probl\u00e8mes", 
          "start": 38.049
        }, 
        {
          "dur": 2.579, 
          "text": "de ce type.", 
          "start": 42.659
        }, 
        {
          "dur": 6.94, 
          "text": "Dans la premi\u00e8re t\u00e2che, nous devons\nentra\u00eener le mod\u00e8le en utilisant seulement X1 et X2,", 
          "start": 45.238
        }, 
        {
          "dur": 1.77, 
          "text": "sans aucune autre extraction de caract\u00e9ristiques.", 
          "start": 52.179
        }, 
        {
          "dur": 8.5, 
          "text": "Pour le premier mod\u00e8le, j&#39;ajoute des tas\nde couches et de neurones dans chacune d&#39;elles.", 
          "start": 53.948
        }, 
        {
          "dur": 6.211, 
          "text": "Plus il y en a, mieux c&#39;est, non\u00a0?", 
          "start": 62.448
        }, 
        {
          "dur": 8.41, 
          "text": "[pause] C&#39;est l\u00e0 le premier pi\u00e8ge.\nIl y a trop de n\u0153uds dans le graphe.", 
          "start": 68.659
        }, 
        {
          "dur": 4.61, 
          "text": "Ce mod\u00e8le est tr\u00e8s lent, car il exige\ndes calculs intensifs pour ajuster toutes", 
          "start": 77.069
        }, 
        {
          "dur": 1.83, 
          "text": "les pond\u00e9rations du graphe\nquand vous avez tant de n\u0153uds.", 
          "start": 81.68
        }, 
        {
          "dur": 3.599, 
          "text": "Vous pouvez le voir avec\nla grande lenteur des it\u00e9rations.", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "Avec tant de couches et de n\u0153uds,\nle mod\u00e8le est aussi difficile \u00e0 interpr\u00e9ter.", 
          "start": 87.109
        }, 
        {
          "dur": 4.44, 
          "text": "La premi\u00e8re couche du mod\u00e8le semble claire,", 
          "start": 92.719
        }, 
        {
          "dur": 3.39, 
          "text": "avec ces caract\u00e9ristiques lin\u00e9aires\net des angles diff\u00e9rents.", 
          "start": 97.159
        }, 
        {
          "dur": 4.541, 
          "text": "Mais si vous examinez le milieu du graphe,\ncertaines d&#39;entre elles ne montrent pas", 
          "start": 100.549
        }, 
        {
          "dur": 6.239, 
          "text": "clairement comment elles contribuent\nau r\u00e9sultat des donn\u00e9es en spirale.", 
          "start": 105.09
        }, 
        {
          "dur": 6.71, 
          "text": "[pause] Comme vous pouvez le voir,\nla perte d&#39;\u00e9valuation s&#39;est am\u00e9lior\u00e9e.", 
          "start": 111.329
        }, 
        {
          "dur": 6.83, 
          "text": "Elle est nettement inf\u00e9rieure \u00e0 50\u00a0%,\nmais la courbe n&#39;est pas encore tr\u00e8s r\u00e9guli\u00e8re.", 
          "start": 118.04
        }, 
        {
          "dur": 4.87, 
          "text": "Voyons si nous pouvons obtenir un mod\u00e8le plus rapide\net plus clair tout en r\u00e9duisant le nombre de n\u0153uds", 
          "start": 124.87
        }, 
        {
          "dur": 7.9, 
          "text": "et aussi avoir une meilleure perte\nd&#39;\u00e9valuation tout en ajustant les param\u00e8tres.", 
          "start": 129.74
        }, 
        {
          "dur": 5.56, 
          "text": "Nous avons maintenant quatre couches cach\u00e9es\ndans le graphe, avec 20\u00a0n\u0153uds au total,", 
          "start": 137.639
        }, 
        {
          "dur": 2.66, 
          "text": "soit moins de la moiti\u00e9\ndes 48 que nous avions pr\u00e9c\u00e9demment.", 
          "start": 143.199
        }, 
        {
          "dur": 2.42, 
          "text": "Ce n&#39;est pas encore un r\u00e9seau\nsuper simple, mais c&#39;est mieux.", 
          "start": 145.86
        }, 
        {
          "dur": 2.28, 
          "text": "L&#39;entra\u00eenement sera plus rapide.", 
          "start": 148.28
        }, 
        {
          "dur": 5.53, 
          "text": "R\u00e9duisons \u00e9galement le taux d&#39;apprentissage\nafin de limiter toutes ces variations", 
          "start": 150.56
        }, 
        {
          "dur": 2.03, 
          "text": "dans la courbe de perte.", 
          "start": 156.09
        }, 
        {
          "dur": 5.83, 
          "text": "Nous obtiendrons peut-\u00eatre une courbe de\nperte plus r\u00e9guli\u00e8re, sans surapprentissage.", 
          "start": 158.12
        }, 
        {
          "dur": 4.31, 
          "text": "Voyons le r\u00e9sultat.", 
          "start": 163.949
        }, 
        {
          "dur": 7.581, 
          "text": "[pause] Avec moins de n\u0153uds et un taux\nd&#39;apprentissage plus lent, le mod\u00e8le apprend", 
          "start": 168.259
        }, 
        {
          "dur": 1.0, 
          "text": "plus vite.", 
          "start": 175.84
        }, 
        {
          "dur": 5.289, 
          "text": "Les it\u00e9rations sont bien plus rapides,\nmais la perte d&#39;\u00e9valuation reste similaire.", 
          "start": 176.84
        }, 
        {
          "dur": 5.021, 
          "text": "Toutefois, les courbes de perte continuent\nd&#39;osciller et le r\u00e9sultat du mod\u00e8le est beaucoup", 
          "start": 182.129
        }, 
        {
          "dur": 3.729, 
          "text": "plus pointu et pas aussi r\u00e9gulier\nque les spirales que nous voudrions apprendre.", 
          "start": 187.15
        }, 
        {
          "dur": 4.771, 
          "text": "Cela nous am\u00e8ne au pi\u00e8ge suivant,\nqui est l&#39;absence de r\u00e9gularisation.", 
          "start": 190.879
        }, 
        {
          "dur": 4.009, 
          "text": "La r\u00e9gularisation p\u00e9nalise les mod\u00e8les\ncomplexes et le surapprentissage.", 
          "start": 195.65
        }, 
        {
          "dur": 5.47, 
          "text": "Voyons si en en ajoutant une, nous obtenons\nune courbe de perte plus r\u00e9guli\u00e8re et un mod\u00e8le", 
          "start": 199.659
        }, 
        {
          "dur": 6.14, 
          "text": "plus performant\nqui n&#39;essaie pas de surapprendre.", 
          "start": 205.129
        }, 
        {
          "dur": 7.41, 
          "text": "Ajoutons cette r\u00e9gularisation au mod\u00e8le\nafin de voir si cela am\u00e9liore la courbe et l&#39;apprentissage.", 
          "start": 211.27
        }, 
        {
          "dur": 5.779, 
          "text": "La r\u00e9gularisation ne doit pas \u00eatre trop \u00e9lev\u00e9e,\ncar elle risquerait sinon de n&#39;apprendre", 
          "start": 218.68
        }, 
        {
          "dur": 2.241, 
          "text": "aucune complexit\u00e9 du mod\u00e8le.", 
          "start": 224.459
        }, 
        {
          "dur": 9.98, 
          "text": "[pause] Comme vous le voyez,\nl&#39;ajout d&#39;une r\u00e9gularisation offre une courbe", 
          "start": 226.699
        }, 
        {
          "dur": 7.75, 
          "text": "de perte plus r\u00e9guli\u00e8re et un r\u00e9sultat\nde mod\u00e8le bien moins pointu.", 
          "start": 236.68
        }, 
        {
          "dur": 2.93, 
          "text": "Sachez que vous pouvez ajuster\nplein d&#39;autres param\u00e8tres du mod\u00e8le.", 
          "start": 244.43
        }, 
        {
          "dur": 3.849, 
          "text": "Vous pouvez modifier la taille du lot,\nmais si elle est trop petite, l&#39;entra\u00eenement", 
          "start": 247.36
        }, 
        {
          "dur": 2.25, 
          "text": "de votre mod\u00e8le risque d&#39;\u00eatre tr\u00e8s long.", 
          "start": 251.209
        }, 
        {
          "dur": 2.67, 
          "text": "Et si la taille est trop grande,\nun probl\u00e8me de convergence peut se poser.", 
          "start": 253.459
        }, 
        {
          "dur": 5.21, 
          "text": "Idem pour le taux d&#39;apprentissage, s&#39;il est\ntrop \u00e9lev\u00e9, cela cause de fortes variations", 
          "start": 256.129
        }, 
        {
          "dur": 1.0, 
          "text": "et un probl\u00e8me de convergence.", 
          "start": 261.339
        }, 
        {
          "dur": 4.859, 
          "text": "Vous pouvez \u00e9galement jouer\navec le taux de r\u00e9gularisation.", 
          "start": 262.339
        }, 
        {
          "dur": 3.951, 
          "text": "En fait, si les donn\u00e9es d&#39;apprentissage \u00e9taient\nmoins bruyantes, le mod\u00e8le serait bien meilleur.", 
          "start": 267.199
        }, 
        {
          "dur": 2.911, 
          "text": "Le bruit est ici de 80.", 
          "start": 271.149
        }, 
        {
          "dur": 5.81, 
          "text": "Si nous le r\u00e9glons sur 30, ou aux environs,\net que nous commen\u00e7ons \u00e0 entra\u00eener le mod\u00e8le,", 
          "start": 274.06
        }, 
        {
          "dur": 2.06, 
          "text": "la perte sera assez bonne.", 
          "start": 279.87
        }, 
        {
          "dur": 4.238, 
          "text": "Toutefois, si le bruit est \u00e9lev\u00e9, une l\u00e9g\u00e8re\nextraction de caract\u00e9ristiques peut aboutir", 
          "start": 281.93
        }, 
        {
          "dur": 5.71, 
          "text": "\u00e0 une am\u00e9lioration bien plus nette que si vous\najoutiez des couches ou ajustiez des param\u00e8tres.", 
          "start": 286.168
        }, 
        {
          "dur": 8.62, 
          "text": "[pause] En r\u00e9duisant le bruit, nous obtenons\ndonc une bien meilleure perte d&#39;\u00e9valuation.", 
          "start": 291.879
        }, 
        {
          "dur": 3.711, 
          "text": "M\u00eame avec seulement de simples caract\u00e9ristiques lin\u00e9aires.\nToutefois, s&#39;il y a du bruit dans les donn\u00e9es,", 
          "start": 300.499
        }, 
        {
          "dur": 6.049, 
          "text": "une extraction de caract\u00e9ristiques est n\u00e9cessaire\npour am\u00e9liorer la perte d&#39;\u00e9valuation.", 
          "start": 304.209
        }, 
        {
          "dur": 2.49, 
          "text": "Le bruit est ici remont\u00e9 \u00e0 80\u00a0%.", 
          "start": 310.259
        }, 
        {
          "dur": 2.361, 
          "text": "Ajoutons des caract\u00e9ristiques plus complexes,", 
          "start": 312.749
        }, 
        {
          "dur": 6.358, 
          "text": "telles que sin, et peut-\u00eatre X1 au carr\u00e9\net X2 au carr\u00e9.", 
          "start": 315.11
        }, 
        {
          "dur": 4.332, 
          "text": "Observons les performances du mod\u00e8le.", 
          "start": 321.468
        }, 
        {
          "dur": 7.928, 
          "text": "[pause] L&#39;ajout de caract\u00e9ristiques plus\ncomplexes montre clairement, notamment", 
          "start": 325.8
        }, 
        {
          "dur": 4.512, 
          "text": "sur la premi\u00e8re couche du mod\u00e8le, que\nles caract\u00e9ristiques sont bien plus complexes.", 
          "start": 333.728
        }, 
        {
          "dur": 5.149, 
          "text": "Les neurones apprennent des choses\nbeaucoup plus complexes que de simples lignes", 
          "start": 338.24
        }, 
        {
          "dur": 2.88, 
          "text": "\u00e0 des pentes diff\u00e9rentes.", 
          "start": 343.389
        }, 
        {
          "dur": 7.471, 
          "text": "En ajoutant ces caract\u00e9ristiques plus complexes,\nnous entra\u00eenons ainsi ces courbes", 
          "start": 346.269
        }, 
        {
          "dur": 1.41, 
          "text": "\u00e0 un rythme beaucoup plus rapide.", 
          "start": 353.74
        }, 
        {
          "dur": 4.759, 
          "text": "Mais vous pouvez voir aussi\nque cette courbe est tr\u00e8s complexe,", 
          "start": 355.149
        }, 
        {
          "dur": 3.892, 
          "text": "ce qui nous ram\u00e8ne au premier pi\u00e8ge\ndu trop grand nombre de n\u0153uds dans le r\u00e9seau.", 
          "start": 359.908
        }, 
        {
          "dur": 5.41, 
          "text": "Essayons de nous d\u00e9barrasser\nd&#39;une partie de ces couches et de ces neurones.", 
          "start": 363.8
        }, 
        {
          "dur": 6.509, 
          "text": "Vous pouvez voir qu&#39;en r\u00e9duisant le nombre\nde couches et de neurones dans chacune d&#39;elles,", 
          "start": 369.209
        }, 
        {
          "dur": 5.911, 
          "text": "et qu&#39;en diminuant le taux d&#39;apprentissage,\nnous obtenons une courbe de perte bien plus", 
          "start": 375.718
        }, 
        {
          "dur": 6.62, 
          "text": "r\u00e9guli\u00e8re et un meilleur apprentissage,\nm\u00eame si du bruit persiste dans les donn\u00e9es.", 
          "start": 381.629
        }, 
        {
          "dur": 5.73, 
          "text": "L&#39;ajout de caract\u00e9ristiques a \u00e9t\u00e9 tr\u00e8s utile,\net en r\u00e9duisant la complexit\u00e9 du mod\u00e8le,", 
          "start": 388.249
        }, 
        {
          "dur": 3.94, 
          "text": "via le nombre de couches dans le mod\u00e8le,\nle r\u00e9sultat est ainsi bien meilleur", 
          "start": 393.978
        }, 
        {
          "dur": 1.291, 
          "text": "et sans surapprentissage.", 
          "start": 397.918
        }, 
        {
          "dur": 5.699, 
          "text": "M\u00eame avec un mod\u00e8le tr\u00e8s simple compos\u00e9\nd&#39;une seule couche cach\u00e9e et de 5\u00a0neurones,", 
          "start": 399.209
        }, 
        {
          "dur": 5.431, 
          "text": "la modification d&#39;autres param\u00e8tres comme\nles taux d&#39;apprentissage et de r\u00e9gularisation,", 
          "start": 404.908
        }, 
        {
          "dur": 7.219, 
          "text": "et la fonction d&#39;activation,\noffre une perte d&#39;\u00e9valuation bien meilleure", 
          "start": 410.339
        }, 
        {
          "dur": 3.81, 
          "text": "et une courbe d&#39;apprentissage plus r\u00e9guli\u00e8re\nque certains des mod\u00e8les les plus complexes.", 
          "start": 417.559
        }, 
        {
          "dur": 1.8, 
          "text": "La convergence est aussi nettement plus rapide.", 
          "start": 421.369
        }
      ]
    }, 
    {
      "lang": "id", 
      "captions": [
        {
          "dur": 3.449, 
          "text": "&gt;&gt; ELIZABETH KEMP: Ini adalah\nlatihan spiral jaringan neural MLCC.", 
          "start": 0.55
        }, 
        {
          "dur": 4.521, 
          "text": "Jika kita menggunakan parameter default\nuntuk model, bisa kita lihat bahwa", 
          "start": 4.409
        }, 
        {
          "dur": 2.77, 
          "text": "hal tersebut tidak\nberperforma dengan baik.", 
          "start": 8.93
        }, 
        {
          "dur": 6.26, 
          "text": "Bahkan setelah 2.500 iterasi, bisa kita\nlihat bahwa kerugian pengujiannya masih", 
          "start": 12.6
        }, 
        {
          "dur": 4.82, 
          "text": "di atas 50%, dan model tidak mempelajari\ndata sama sekali.", 
          "start": 18.86
        }, 
        {
          "dur": 3.16, 
          "text": "Mari kita lihat apa kita bisa\nmelakukannya dengan lebih baik.", 
          "start": 23.68
        }, 
        {
          "dur": 5.629, 
          "text": "Meskipun data terlihat kompleks di \njalinan spiral ini, kita tidak memerlukan", 
          "start": 27.04
        }, 
        {
          "dur": 3.211, 
          "text": "model yang sangat kompleks atau dalam\nuntuk memecahkan masalahnya", 
          "start": 32.669
        }, 
        {
          "dur": 3.05, 
          "text": "jika kita memiliki fitur dan\nparameter yang tepat.", 
          "start": 35.88
        }, 
        {
          "dur": 2.83, 
          "text": "Saya akan membahas beberapa masalah\nyang perlu diperhatikan", 
          "start": 38.93
        }, 
        {
          "dur": 2.289, 
          "text": "saat memecahkan masalah seperti ini.", 
          "start": 41.76
        }, 
        {
          "dur": 6.29, 
          "text": "Tugas pertama meminta kita untuk melatih\nmodel hanya menggunakan X1 dan X2,", 
          "start": 44.889
        }, 
        {
          "dur": 2.77, 
          "text": "tanpa rekayasa fitur tambahan.", 
          "start": 51.179
        }, 
        {
          "dur": 3.98, 
          "text": "Di model pertama, saya akan coba\ntambahkan banyak lapisan", 
          "start": 53.949
        }, 
        {
          "dur": 4.04, 
          "text": "dan banyak neuron di setiap lapisan.", 
          "start": 57.929
        }, 
        {
          "dur": 3.181, 
          "text": "Karena semakin banyak\nakan semakin bagus.", 
          "start": 61.969
        }, 
        {
          "dur": 5.84, 
          "text": "Ini adalah masalah pertama kita,\nterlalu banyak simpul di dalam graf.", 
          "start": 71.23
        }, 
        {
          "dur": 4.0, 
          "text": "Model ini sangat lambat, mahal dalam\nhal komputasi untuk menyesuaikan", 
          "start": 77.07
        }, 
        {
          "dur": 2.44, 
          "text": "semua beban di graf,\njika simpulnya sangat banyak.", 
          "start": 81.07
        }, 
        {
          "dur": 3.6, 
          "text": "Bisa kalian lihat, proses penghitungan\niterasinya sangat lambat.", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "Dan juga, model tidak mudah ditafsirkan\nkarena banyak lapisan dan simpul di graf.", 
          "start": 87.11
        }, 
        {
          "dur": 4.0, 
          "text": "Lapisan pertama dari model\nterbilang masih masuk akal.", 
          "start": 92.72
        }, 
        {
          "dur": 3.52, 
          "text": "Ini adalah fitur linear\ndi sudut yang berbeda.", 
          "start": 96.72
        }, 
        {
          "dur": 4.84, 
          "text": "Tapi jika kalian beralih ke bagian tengah\ngraf, beberapa fitur ini tidak terlalu", 
          "start": 100.25
        }, 
        {
          "dur": 6.24, 
          "text": "jelas kontribusinya terhadap\nhasil data spiral tersebut.", 
          "start": 105.09
        }, 
        {
          "dur": 3.39, 
          "text": "Seperti yang bisa kalian lihat,\nkerugian pengujian kita telah meningkat.", 
          "start": 114.65
        }, 
        {
          "dur": 6.83, 
          "text": "Sekarang kerugian kita jelas berada di\nbawah 50%, tapi kurva masih belum halus.", 
          "start": 118.04
        }, 
        {
          "dur": 3.72, 
          "text": "Mari kita lihat apa jumlah simpul bisa\ndikurangi dan apa kita bisa mendapatkan", 
          "start": 124.87
        }, 
        {
          "dur": 1.15, 
          "text": "model yang lebih cepat", 
          "start": 128.59
        }, 
        {
          "dur": 4.44, 
          "text": "yang membuatnya lebih masuk akal dan\nmenyesuaikan parameter", 
          "start": 129.74
        }, 
        {
          "dur": 3.08, 
          "text": "untuk mendapatkan kerugian pengujian\nyang lebih baik.", 
          "start": 134.18
        }, 
        {
          "dur": 4.4, 
          "text": "Sekarang kita punya 4 lapisan tersembunyi\ndi graf dengan total 20 simpul, yaitu", 
          "start": 137.64
        }, 
        {
          "dur": 3.45, 
          "text": "kurang dari setengah jumlah\nsimpul sebelumnya, yaitu 48.", 
          "start": 142.04
        }, 
        {
          "dur": 3.05, 
          "text": "Ini bukan jaringan yang sangat\nsederhana, tetapi ini lebih baik.", 
          "start": 145.49
        }, 
        {
          "dur": 2.02, 
          "text": "Jaringan ini akan melatih lebih cepat.", 
          "start": 148.54
        }, 
        {
          "dur": 5.14, 
          "text": "Dan saya juga akan mengurangi kecepatan\npemelajaran, sehingga kurva kerugian", 
          "start": 150.56
        }, 
        {
          "dur": 2.28, 
          "text": "yang didapat tidak akan bergerak\nmaju mundur.", 
          "start": 155.7
        }, 
        {
          "dur": 3.58, 
          "text": "Semoga ini akan memberikan kurva kerugian\nyang lebih halus dan tidak overfit", 
          "start": 157.98
        }, 
        {
          "dur": 1.82, 
          "text": "bagi data pengujian.", 
          "start": 161.56
        }, 
        {
          "dur": 2.38, 
          "text": "Mari kita lihat performanya.", 
          "start": 163.95
        }, 
        {
          "dur": 3.39, 
          "text": "Dengan lebih sedikit simpul dan\nkecepatan pemelajaran yang lebih lambat,", 
          "start": 171.3
        }, 
        {
          "dur": 2.15, 
          "text": "bisa dilihat model belajar\ndengan lebih cepat.", 
          "start": 174.69
        }, 
        {
          "dur": 5.29, 
          "text": "Iterasi berjalan jauh lebih cepat,\ntetapi kerugian pengujian masih sama.", 
          "start": 176.84
        }, 
        {
          "dur": 5.02, 
          "text": "Tapi seperti yang terlihat, kurva kerugian\nmasih bergerak dan keluaran model menjadi", 
          "start": 182.13
        }, 
        {
          "dur": 3.73, 
          "text": "lebih runcing, dan tidak halus, seperti\nspiral yang sedang kita coba sesuaikan.", 
          "start": 187.15
        }, 
        {
          "dur": 4.77, 
          "text": "Dan hal itu mengarahkan kita ke masalah\nberikutnya, yaitu tidak ada regularisasi.", 
          "start": 190.88
        }, 
        {
          "dur": 4.01, 
          "text": "Jadi, regularisasi mengganjar\nmodel kompleks dan overfitting.", 
          "start": 195.65
        }, 
        {
          "dur": 5.12, 
          "text": "Jadi, jika kita tambahkan ini, kita bisa\ndapatkan kurva kerugian yang lebih halus", 
          "start": 199.66
        }, 
        {
          "dur": 2.18, 
          "text": "dan model yang lebih sesuai", 
          "start": 204.78
        }, 
        {
          "dur": 3.51, 
          "text": "yang tidak mencoba melakukan\noverfitting pada data pelatihan.", 
          "start": 206.96
        }, 
        {
          "dur": 4.39, 
          "text": "Mari kita tambahkan regularisasi ke model\nuntuk melihat apakah kurva dan", 
          "start": 211.27
        }, 
        {
          "dur": 3.02, 
          "text": "penyesuaiannya bisa menjadi lebih baik.", 
          "start": 215.66
        }, 
        {
          "dur": 5.259, 
          "text": "Pastikan regularisasi tidak terlalu tinggi\nsehingga regularisasi tidak mempelajari", 
          "start": 218.68
        }, 
        {
          "dur": 2.761, 
          "text": "kompleksitas apa pun di dalam model.", 
          "start": 223.939
        }, 
        {
          "dur": 5.42, 
          "text": "Bisa lihat bahwa dengan menambahkan\nregularisasi, kita membuat kurva kerugian", 
          "start": 230.1
        }, 
        {
          "dur": 6.88, 
          "text": "menjadi lebih halus dan keluaran model\nmenjadi tidak runcing.", 
          "start": 235.52
        }, 
        {
          "dur": 3.48, 
          "text": "Jadi, ada banyak parameter lain yang dapat\nAnda gunakan dalam model.", 
          "start": 243.88
        }, 
        {
          "dur": 4.059, 
          "text": "Anda bisa ubah ukuran tumpukan, dan jika\nterlalu kecil, proses pelatihan model akan", 
          "start": 247.36
        }, 
        {
          "dur": 2.04, 
          "text": "memakan waktu yang lama.", 
          "start": 251.419
        }, 
        {
          "dur": 3.281, 
          "text": "Jika terlalu besar, mungkin akan kesulitan\nsaat melakukan konvergensi.", 
          "start": 253.459
        }, 
        {
          "dur": 3.86, 
          "text": "Sama kecepatan pemelajaran, jika terlalu\ncepat, model akan terlalu banyak bergerak", 
          "start": 256.74
        }, 
        {
          "dur": 1.88, 
          "text": "dan sulit melakukan konvergensi.", 
          "start": 260.6
        }, 
        {
          "dur": 3.659, 
          "text": "Selain itu, kalian juga bisa ubah\nderajat regularisasi.", 
          "start": 262.48
        }, 
        {
          "dur": 3.951, 
          "text": "Model akan menjadi baik jika\nderau pada data pelatihan hanya sedikit.", 
          "start": 267.199
        }, 
        {
          "dur": 2.91, 
          "text": "Di sini deraunya disetel di 80.", 
          "start": 271.15
        }, 
        {
          "dur": 5.16, 
          "text": "Jika kita turunkan menjadi 30,\ndan mulai melatih model ini,", 
          "start": 274.06
        }, 
        {
          "dur": 2.71, 
          "text": "kerugiannya akan jadi lebih baik.", 
          "start": 279.22
        }, 
        {
          "dur": 2.009, 
          "text": "Tapi dengan tingkat derau yang tinggi,", 
          "start": 281.93
        }, 
        {
          "dur": 2.29, 
          "text": "kadang, dengan menambahkan\nsedikit rekayasa fitur", 
          "start": 283.939
        }, 
        {
          "dur": 3.15, 
          "text": "akan memberi kita peningkatan yang\nlebih besar daripada menambahkan", 
          "start": 286.229
        }, 
        {
          "dur": 2.48, 
          "text": "lebih banyak lapisan atau\nmenyempurnakan parameter.", 
          "start": 289.379
        }, 
        {
          "dur": 5.06, 
          "text": "Lihatlah, dengan mengurangi derau,\nkita bisa dapatkan kerugian pengujian", 
          "start": 293.909
        }, 
        {
          "dur": 1.53, 
          "text": "yang jauh lebih baik.", 
          "start": 298.969
        }, 
        {
          "dur": 3.601, 
          "text": "Bahkan hanya menggunakan fitur linear,\ntapi jika kita punya data yang rusak,", 
          "start": 300.499
        }, 
        {
          "dur": 2.619, 
          "text": "kita harus melakukan rekayasa fitur", 
          "start": 304.1
        }, 
        {
          "dur": 2.84, 
          "text": "untuk mendapatkan\nkerugian pengujian yang lebih baik.", 
          "start": 306.719
        }, 
        {
          "dur": 2.63, 
          "text": "Jadi, kita kembalikan derau ke 80%.", 
          "start": 310.119
        }, 
        {
          "dur": 2.361, 
          "text": "Mari kita tambahkan beberapa\nfitur kompleks.", 
          "start": 312.749
        }, 
        {
          "dur": 6.359, 
          "text": "Mari coba tambahkan sin dan X1 kuadrat\ndan X2 kuadrat mungkin.", 
          "start": 315.11
        }, 
        {
          "dur": 2.951, 
          "text": "Mari kita lihat performa model ini.", 
          "start": 321.469
        }, 
        {
          "dur": 4.019, 
          "text": "Hal mencolok yang kalian lihat dengan\nmenambahkan fitur kompleks adalah,", 
          "start": 329.25
        }, 
        {
          "dur": 4.971, 
          "text": "khususnya di lapisan pertama dari model,\nfitur menjadi jauh lebih kompleks.", 
          "start": 333.269
        }, 
        {
          "dur": 4.189, 
          "text": "Hal yang dipelajari neuron menjadi jauh\nlebih kompleks daripada hanya garis di", 
          "start": 338.24
        }, 
        {
          "dur": 2.441, 
          "text": "kemiringan yang berbeda.", 
          "start": 342.429
        }, 
        {
          "dur": 5.25, 
          "text": "Bisa lihat bahwa dengan menambahkan\nlebih banyak fitur kompleks,", 
          "start": 346.27
        }, 
        {
          "dur": 3.58, 
          "text": "kita mempelajari kurva ini\ndengan lebih cepat.", 
          "start": 351.52
        }, 
        {
          "dur": 4.479, 
          "text": "Tapi kalian masih bisa lihat bahwa kurva\nini sangat kompleks, dan hal itu kembali", 
          "start": 355.1
        }, 
        {
          "dur": 4.221, 
          "text": "ke masalah kita yang pertama, yaitu\nterlalu banyak simpul di jaringan.", 
          "start": 359.579
        }, 
        {
          "dur": 5.09, 
          "text": "Mari kita coba hapus\nbeberapa lapisan dan neuron ini.", 
          "start": 363.8
        }, 
        {
          "dur": 5.559, 
          "text": "Kalian bisa lihat, dengan mengurangi\njumlah lapisan dan jumlah neuron", 
          "start": 369.21
        }, 
        {
          "dur": 4.05, 
          "text": "di setiap lapisan, saya juga turunkan\nkecepatan pemelajarannya,", 
          "start": 374.769
        }, 
        {
          "dur": 2.86, 
          "text": "kurva kerugian menjadi lebih halus", 
          "start": 378.819
        }, 
        {
          "dur": 6.57, 
          "text": "dan menyesuaikan data dengan lebih baik,\nmeski masih banyak data yang rusak.", 
          "start": 381.679
        }, 
        {
          "dur": 5.55, 
          "text": "Fitur yang kita tambahkan sangat membantu,\ndan dengan mengurangi kompleksitas model,", 
          "start": 388.249
        }, 
        {
          "dur": 3.39, 
          "text": "jumlah lapisan di model, kita telah\nmelakukan upaya yang jauh lebih baik", 
          "start": 393.799
        }, 
        {
          "dur": 2.201, 
          "text": "untuk tidak membuat data\nmengalami overfitting.", 
          "start": 397.189
        }, 
        {
          "dur": 3.594, 
          "text": "Bisa kalian lihat bahwa bahkan\ndengan model yang sangat sederhana,", 
          "start": 399.39
        }, 
        {
          "dur": 3.02, 
          "text": "hanya satu lapisan tersembunyi\ndan lima neuron,", 
          "start": 402.984
        }, 
        {
          "dur": 2.756, 
          "text": "dengan mengubah beberapa parameter lain,", 
          "start": 406.004
        }, 
        {
          "dur": 3.78, 
          "text": "seperti kecepatan pemelajaran dan\nderajat regularisasi serta fungsi aktivasi", 
          "start": 408.76
        }, 
        {
          "dur": 3.149, 
          "text": "saya mendapatkan kerugian pengujian\nyang lebih baik serta", 
          "start": 412.54
        }, 
        {
          "dur": 3.45, 
          "text": "kurva penyesuaian yang\nlebih halus daripada", 
          "start": 415.689
        }, 
        {
          "dur": 2.23, 
          "text": "beberapa model kompleks lainnya.", 
          "start": 419.139
        }, 
        {
          "dur": 3.18, 
          "text": "Dan proses konvergensinya\nberjalan jauh lebih cepat.", 
          "start": 421.369
        }
      ]
    }, 
    {
      "lang": "ko", 
      "captions": [
        {
          "dur": 3.079, 
          "text": "&gt;&gt; Elizabeth Kemp: MLCC\n\ub098\uc120\ud615 \uc2e0\uacbd\ub9dd \uc5f0\uc2b5\uc774\uc5d0\uc694.", 
          "start": 0.55
        }, 
        {
          "dur": 6.72, 
          "text": "\ubaa8\ub378\uc5d0 \uae30\ubcf8 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc0ac\uc6a9\ud558\uba74\n\ubcf4\uc2dc\ub2e4\uc2dc\ud53c \uacb0\uacfc\uac00 \uadf8\ub2e4\uc9c0", 
          "start": 3.629
        }, 
        {
          "dur": 2.811, 
          "text": "\uc88b\uc9c0 \uc54a\uc8e0.", 
          "start": 10.349
        }, 
        {
          "dur": 7.679, 
          "text": "2,500\ud68c\ub97c \ubc18\ubcf5\ud574\ub3c4\n\ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc774 50%\uac00 \ub118\uace0", 
          "start": 13.16
        }, 
        {
          "dur": 2.84, 
          "text": "\ubaa8\ub378\uc774 \ub370\uc774\ud130\ub97c \uc804\ud600 \ud559\uc2b5\ud558\uc9c0 \ubabb\ud574\uc694.", 
          "start": 20.839
        }, 
        {
          "dur": 3.821, 
          "text": "\uac1c\uc120\uc758 \uc5ec\uc9c0\uac00 \uc788\ub294\uc9c0 \uc54c\uc544\ubcf4\uc8e0.", 
          "start": 23.679
        }, 
        {
          "dur": 5.168, 
          "text": "\ub4a4\uc5bd\ud600 \uc788\ub294 \ub098\uc120\ud615\uc744 \ubcf4\uba74\n\ub370\uc774\ud130\uac00 \ub108\ubb34 \ubcf5\uc7a1\ud574 \ubcf4\uc774\uc9c0\ub9cc", 
          "start": 27.5
        }, 
        {
          "dur": 4.381, 
          "text": "\ud2b9\uc131\uacfc \ub9e4\uac1c\ubcc0\uc218\ub9cc \uc801\uc808\ud558\ub2e4\uba74\n\uc544\uc8fc \uc5b4\ub835\uac70\ub098 \ubcf5\uc7a1\ud55c \ubaa8\ub378 \uc5c6\uc774\ub3c4", 
          "start": 32.668
        }, 
        {
          "dur": 1.0, 
          "text": "\ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\uc5b4\uc694.", 
          "start": 37.049
        }, 
        {
          "dur": 4.61, 
          "text": "\uc774\ub7f0 \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \ub54c\n\uc8fc\uc758\ud574\uc57c \ud560 \ud568\uc815 \uba87 \uac00\uc9c0\ub97c", 
          "start": 38.049
        }, 
        {
          "dur": 2.579, 
          "text": "\uc54c\ub824\ub4dc\ub9b4\uac8c\uc694.", 
          "start": 42.659
        }, 
        {
          "dur": 6.94, 
          "text": "\uba3c\uc800 \ud560 \uc77c\uc740 \ucd94\uac00 \ud2b9\uc131 \ucd94\ucd9c \uc5c6\uc774\nX1\uacfc X2\ub9cc \uc0ac\uc6a9\ud574 \ubaa8\ub378\uc744", 
          "start": 45.238
        }, 
        {
          "dur": 1.77, 
          "text": "\ud559\uc2b5\uc2dc\ud0a4\ub294 \uac70\uc608\uc694.", 
          "start": 52.179
        }, 
        {
          "dur": 8.5, 
          "text": "\uccab \ubc88\uc9f8 \ubaa8\ub378\uc5d0\ub294 \ub808\uc774\uc5b4\ub97c \uc5ec\ub7ec \uac1c \ucd94\uac00\ud558\uace0\n\uac01 \ub808\uc774\uc5b4\uc5d0\ub294 \ub274\ub7f0\uc744 \uc5ec\ub7ec \uac1c \ucd94\uac00\ud574 \ubcfc\uac8c\uc694.", 
          "start": 53.948
        }, 
        {
          "dur": 6.211, 
          "text": "\ub9ce\uc744\uc218\ub85d \uc88b\uc740 \uac70\uaca0\uc8e0?", 
          "start": 62.448
        }, 
        {
          "dur": 8.41, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \uadf8\ub798\ud504\uc5d0 \ub178\ub4dc\uac00\n\ub108\ubb34 \ub9ce\uc740 \uac8c \ubc14\ub85c \uccab \ubc88\uc9f8 \ud568\uc815\uc774\uc5d0\uc694.", 
          "start": 68.659
        }, 
        {
          "dur": 4.61, 
          "text": "\ub178\ub4dc\uac00 \ub9ce\uc73c\uba74 \uadf8\ub798\ud504\uc5d0\uc11c \ubaa8\ub4e0 \uac00\uc911\uce58\ub97c\n\uc870\uc815\ud558\ub294 \ub370 \uc5f0\uc0b0\uc774 \ubcf5\uc7a1\ud574\uc838\uc11c", 
          "start": 77.069
        }, 
        {
          "dur": 1.83, 
          "text": "\ubaa8\ub378 \uc18d\ub3c4\uac00 \ub290\ub824\uc9c0\uac8c \ub3fc\uc694.", 
          "start": 81.68
        }, 
        {
          "dur": 3.599, 
          "text": "\ubc18\ubcf5 \ud69f\uc218\uac00 \uc544\uc8fc \ub290\ub9ac\uac8c \uc62c\ub77c\uac00\uace0 \uc788\uc8e0.", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "\uadf8\ub798\ud504\uc5d0 \ub808\uc774\uc5b4\uc640 \ub178\ub4dc\uac00 \ub108\ubb34 \ub9ce\uc73c\uba74\n\ubaa8\ub378\uc744 \ud574\uc11d\ud558\uae30\ub3c4 \uc5b4\ub835\uac8c \ub3fc\uc694.", 
          "start": 87.109
        }, 
        {
          "dur": 4.44, 
          "text": "\ubaa8\ub378\uc758 \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\ub294\n\uc5b4\ub290 \uc815\ub3c4 \ud574\uc11d\ud560 \uc218 \uc788\ub124\uc694.", 
          "start": 92.719
        }, 
        {
          "dur": 3.39, 
          "text": "\uc5ec\ub7ec \uac01\ub3c4\uc5d0\uc11c\n\uc120\ud615 \ud2b9\uc131\uc774 \ub098\ud0c0\ub098\uace0 \uc788\uc5b4\uc694.", 
          "start": 97.159
        }, 
        {
          "dur": 4.541, 
          "text": "\uadf8\ub7f0\ub370 \uadf8\ub798\ud504\uc758 \uc911\uac04\uc73c\ub85c \uac08\uc218\ub85d\n\uc77c\ubd80 \ud2b9\uc131\uc774 \ub098\uc120 \ub370\uc774\ud130 \uacb0\uacfc\ubb3c\uc5d0", 
          "start": 100.549
        }, 
        {
          "dur": 6.239, 
          "text": "\uc5b4\ub5a4 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294\uc9c0 \uc54c \uc218\uac00 \uc5c6\uc8e0.", 
          "start": 105.09
        }, 
        {
          "dur": 6.71, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \uc774\uc81c \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc774 \uac1c\uc120\ub418\uc5c8\ub124\uc694.", 
          "start": 111.329
        }, 
        {
          "dur": 6.83, 
          "text": "\uc190\uc2e4\uc774 50% \ubbf8\ub9cc\uc774\uae30\ub294 \ud574\ub3c4\n\uc544\uc9c1 \ubd80\ub4dc\ub7ec\uc6b4 \uace1\uc120\uc740 \uc544\ub2c8\uc5d0\uc694.", 
          "start": 118.04
        }, 
        {
          "dur": 4.87, 
          "text": "\ub178\ub4dc \uc218\ub97c \uc904\uc5ec\uc11c \ub354 \uc774\ud574\ud558\uae30 \uc27d\uace0\n\ube60\ub978 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uace0", 
          "start": 124.87
        }, 
        {
          "dur": 7.9, 
          "text": "\ub9e4\uac1c\ubcc0\uc218\ub97c \uc870\uc808\ud574\uc11c \ub354 \ub098\uc740\n\ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc744 \uc5bb\uc744 \uc218 \uc788\uc744\uc9c0 \uc54c\uc544\ubcf4\uc8e0.", 
          "start": 129.74
        }, 
        {
          "dur": 5.56, 
          "text": "\uc774\uc81c \uadf8\ub798\ud504\uc5d0 \ud788\ub4e0 \ub808\uc774\uc5b4 4\uac1c\uc640\n\ucd1d 20\uac1c\uc758 \ub178\ub4dc\uac00 \uc788\ub124\uc694.", 
          "start": 137.639
        }, 
        {
          "dur": 2.66, 
          "text": "\uc608\uc804\uc758 48\uac1c\uc640 \ube44\uad50\ud558\uba74 \uc808\ubc18\ub3c4 \uc548 \ub418\uc8e0.", 
          "start": 143.199
        }, 
        {
          "dur": 2.42, 
          "text": "\uc544\uc9c1 \ub9dd\uc774 \uadf8\ub807\uac8c \ub2e8\uc21c\ud558\uc9c0\ub294 \uc54a\uc9c0\ub9cc\n\ud6e8\uc52c \ub098\uc544\uc84c\uc5b4\uc694.", 
          "start": 145.86
        }, 
        {
          "dur": 2.28, 
          "text": "\ud559\uc2b5 \uc18d\ub3c4\ub3c4 \ube68\ub77c\uc9c8 \uac70\uace0\uc694.", 
          "start": 148.28
        }, 
        {
          "dur": 5.53, 
          "text": "\uc190\uc2e4 \uace1\uc120\uc774 \uc704\uc544\ub798\ub85c \uae09\uaca9\ud558\uac8c\n\ub2ec\ub77c\uc9c0\uc9c0 \uc54a\ub3c4\ub85d", 
          "start": 150.56
        }, 
        {
          "dur": 2.03, 
          "text": "\ud559\uc2b5\ub960\uc744 \ub0ae\ucdb0\ubcf4\uc8e0.", 
          "start": 156.09
        }, 
        {
          "dur": 5.83, 
          "text": "\uace1\uc120\uc774 \ubd80\ub4dc\ub7ec\uc6cc\uc9c0\uace0 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0\n\uacfc\uc801\ud569\ub418\uc9c0 \uc54a\uae30\ub97c \ubc14\ub77c\uba74\uc11c\uc694.", 
          "start": 158.12
        }, 
        {
          "dur": 4.31, 
          "text": "\uc774\uc81c \uacb0\uacfc\ub97c \uc0b4\ud3b4\ubcf4\uc8e0.", 
          "start": 163.949
        }, 
        {
          "dur": 7.581, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \ub178\ub4dc \uc218\uc640 \ud559\uc2b5\ub960\uc744 \uc904\uc774\ub2c8\n\ubaa8\ub378\uc758 \ud559\uc2b5 \uc18d\ub3c4\uac00", 
          "start": 168.259
        }, 
        {
          "dur": 1.0, 
          "text": "\ube68\ub77c\uc9c0\ub124\uc694.", 
          "start": 175.84
        }, 
        {
          "dur": 5.289, 
          "text": "\ubc18\ubcf5\uc774 \ud6e8\uc52c \ube60\ub974\uac8c \uc9c4\ud589\ub418\uc9c0\ub9cc\n\ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc740 \uc544\uc9c1 \ube44\uc2b7\ud574\uc694.", 
          "start": 176.84
        }, 
        {
          "dur": 5.021, 
          "text": "\uc190\uc2e4 \uace1\uc120\uc740 \uc544\uc9c1\ub3c4 \uc591\uadf9\uc744 \uc624\uac00\uace0 \uc788\uace0\n\ubaa8\ub378 \ucd9c\ub825\uc740 \uc6d0\ud558\ub358 \uac83\ucc98\ub7fc \ubd80\ub4dc\ub7ec\uc6b4", 
          "start": 182.129
        }, 
        {
          "dur": 3.729, 
          "text": "\ub098\uc120\uc774 \uc544\ub2cc \ubfb0\uc871\ud55c \ubaa8\uc591\uc774\ub124\uc694.", 
          "start": 187.15
        }, 
        {
          "dur": 4.771, 
          "text": "\uc815\uaddc\ud654\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \uac8c\n\ubc14\ub85c \ub450 \ubc88\uc9f8 \ud568\uc815\uc774\uc5d0\uc694.", 
          "start": 190.879
        }, 
        {
          "dur": 4.009, 
          "text": "\uc815\uaddc\ud654\ub294 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uacfc \uacfc\uc801\ud569\uc5d0\n\ud398\ub110\ud2f0\ub97c \uc8fc\uc8e0.", 
          "start": 195.65
        }, 
        {
          "dur": 5.47, 
          "text": "\uc815\uaddc\ud654\ub97c \ucd94\uac00\ud574\uc11c \ubd80\ub4dc\ub7ec\uc6b4 \uc190\uc2e4 \uace1\uc120\uacfc\n\ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \uacfc\uc801\ud569\ub418\uc9c0 \uc54a\ub294", 
          "start": 199.659
        }, 
        {
          "dur": 6.14, 
          "text": "\ub354 \uc801\ud569\ud55c \ubaa8\ub378\uc744\n\uc5bb\uc744 \uc218 \uc788\uc744\uc9c0 \uc54c\uc544\ubcf4\uc8e0.", 
          "start": 205.129
        }, 
        {
          "dur": 7.41, 
          "text": "\ubaa8\ub378\uc5d0 \uc815\uaddc\ud654\ub97c \ucd94\uac00\ud574\uc11c \uace1\uc120\uacfc\n\uc801\ud569\uc131\uc774 \uac1c\uc120\ub418\ub294\uc9c0 \uc54c\uc544\ubcfc\uac8c\uc694.", 
          "start": 211.27
        }, 
        {
          "dur": 5.779, 
          "text": "\uc815\uaddc\ud654 \uc218\uc900\uc774 \ub108\ubb34 \ub192\uc9c0 \uc54a\uac8c \ud558\uace0\n\ubaa8\ub378\uc758 \ubcf5\uc7a1\ub3c4\ub97c \ud559\uc2b5\ud558\uc9c0 \uc54a\ub3c4\ub85d", 
          "start": 218.68
        }, 
        {
          "dur": 2.241, 
          "text": "\ud558\uaca0\uc2b5\ub2c8\ub2e4.", 
          "start": 224.459
        }, 
        {
          "dur": 9.98, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \uc815\uaddc\ud654\ub97c \ucd94\uac00\ud558\ub2c8\n\uc190\uc2e4 \uace1\uc120\uc774 \ub354\uc6b1 \ubd80\ub4dc\ub7ec\uc6cc\uc9c0\uace0", 
          "start": 226.699
        }, 
        {
          "dur": 7.75, 
          "text": "\ubaa8\ub378 \ucd9c\ub825\uc740 \ub35c \ubfb0\uc871\ud558\uac8c \ub418\uc5c8\ub124\uc694.", 
          "start": 236.68
        }, 
        {
          "dur": 2.93, 
          "text": "\ubaa8\ub378\uc5d0\uc11c \uc2dc\ub3c4\ud574 \ubcfc \uc218 \uc788\ub294\n\ub2e4\ub978 \ub9e4\uac1c\ubcc0\uc218\ub3c4 \ub9ce\uc740\ub370", 
          "start": 244.43
        }, 
        {
          "dur": 3.849, 
          "text": "\ubc30\uce58 \ud06c\uae30\ub97c \ubcc0\uacbd\ud574 \ubcfc \uc218 \uc788\uc5b4\uc694.\n\ud06c\uae30\uac00 \ub108\ubb34 \uc791\uc73c\uba74 \ubaa8\ub378\uc774 \ud559\uc2b5\ud558\ub294 \ub370", 
          "start": 247.36
        }, 
        {
          "dur": 2.25, 
          "text": "\uc2dc\uac04\uc774 \ub108\ubb34 \ub9ce\uc774 \uac78\ub9ac\uc8e0.", 
          "start": 251.209
        }, 
        {
          "dur": 2.67, 
          "text": "\ubc18\ub300\ub85c \ub108\ubb34 \ud06c\uba74 \uc218\ub834\uc5d0\n\ubb38\uc81c\uac00 \uc0dd\uae38 \uc218\ub3c4 \uc788\uace0\uc694.", 
          "start": 253.459
        }, 
        {
          "dur": 5.21, 
          "text": "\ud559\uc2b5\ub960\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ub108\ubb34 \ub192\uc73c\uba74\n\ubcc0\ub3d9\uc774 \ub9ce\uc544\uc9c0\uace0 \uc218\ub834\uc5d0 \ubb38\uc81c\uac00", 
          "start": 256.129
        }, 
        {
          "dur": 1.0, 
          "text": "\uc0dd\uae38 \uc218 \uc788\uc5b4\uc694.", 
          "start": 261.339
        }, 
        {
          "dur": 4.859, 
          "text": "\uc815\uaddc\ud654\uc728\uc5d0\ub3c4 \uc774\ub7f0 \uc2dc\ub3c4\ub97c \ud574\ubcfc \uc218 \uc788\uc8e0.", 
          "start": 262.339
        }, 
        {
          "dur": 3.951, 
          "text": "\ud559\uc2b5 \ub370\uc774\ud130\uc5d0 \ub178\uc774\uc988\uac00 \uc801\uc5c8\ub2e4\uba74\n\ud6e8\uc52c \uc88b\uc740 \ubaa8\ub378\uc774\uc5c8\uc744 \uac70\uc608\uc694.", 
          "start": 267.199
        }, 
        {
          "dur": 2.911, 
          "text": "\uc5ec\uae30\uc5d0\uc11c\ub294 \ub178\uc774\uc988\ub97c 80\uae4c\uc9c0 \uc62c\ub838\ub294\ub370", 
          "start": 271.149
        }, 
        {
          "dur": 5.81, 
          "text": "\ub178\uc774\uc988\ub97c 30 \uc815\ub3c4\ub85c \ub0ae\ucd94\uace0\n\ubaa8\ub378 \ud559\uc2b5\uc744 \uc2dc\uc791\ud558\uba74 \uc2e4\uc81c\ub85c", 
          "start": 274.06
        }, 
        {
          "dur": 2.06, 
          "text": "\uc190\uc2e4\uc774 \uac1c\uc120\ub429\ub2c8\ub2e4.", 
          "start": 279.87
        }, 
        {
          "dur": 4.238, 
          "text": "\ud558\uc9c0\ub9cc \ub178\uc774\uc988 \uc218\uc900\uc774 \ub192\uc744 \ub54c\ub294\n\uc57d\uac04\uc758 \ud2b9\uc131 \ucd94\ucd9c\ub9cc\uc73c\ub85c\ub3c4", 
          "start": 281.93
        }, 
        {
          "dur": 5.71, 
          "text": "\ub808\uc774\uc5b4 \ucd94\uac00\ub098 \ub9e4\uac1c\ubcc0\uc218 \uc870\uc815\ubcf4\ub2e4\n\ud06c\uac8c \uacb0\uacfc\ub97c \uac1c\uc120\ud560 \uc218 \uc788\uc5b4\uc694.", 
          "start": 286.168
        }, 
        {
          "dur": 8.62, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \ub178\uc774\uc988\ub97c \uc904\uc774\ub2c8\n\ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc774 \ud6e8\uc52c \uac1c\uc120\ub418\uc5c8\ub124\uc694.", 
          "start": 291.879
        }, 
        {
          "dur": 3.711, 
          "text": "\uc120\ud615 \ud2b9\uc131\ub9cc \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0\uc5d0\ub3c4\n\ub370\uc774\ud130\uc758 \ub178\uc774\uc988 \uc218\uc900\uc774 \uc774 \uc815\ub3c4\uba74", 
          "start": 300.499
        }, 
        {
          "dur": 6.049, 
          "text": "\ub354 \ub098\uc740 \ud14c\uc2a4\ud2b8 \uc190\uc2e4\uc744 \uc5bb\uae30 \uc704\ud574\n\ud2b9\uc131 \ucd94\ucd9c \uc791\uc5c5\uc744 \ud574\uc57c \ud558\uc8e0.", 
          "start": 304.209
        }, 
        {
          "dur": 2.49, 
          "text": "\ub178\uc774\uc988\ub97c \ub2e4\uc2dc 80%\ub85c \uc62c\ub9ac\uace0", 
          "start": 310.259
        }, 
        {
          "dur": 2.361, 
          "text": "\ub354 \ubcf5\uc7a1\ud55c \ud2b9\uc131\uc744 \ucd94\uac00\ud574 \ubcf4\uc8e0.", 
          "start": 312.749
        }, 
        {
          "dur": 6.358, 
          "text": "sin\uc744 \ucd94\uac00\ud558\uace0\nX1 \uc81c\uacf1\uacfc X2 \uc81c\uacf1\ub3c4 \ub123\uc5b4\ubcf4\uc8e0.", 
          "start": 315.11
        }, 
        {
          "dur": 4.332, 
          "text": "\ubaa8\ub378\uc774 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub294\uc9c0 \ubcfc\uae4c\uc694?", 
          "start": 321.468
        }, 
        {
          "dur": 7.928, 
          "text": "[\uc7a0\uc2dc \uba48\ucda4] \ub354 \ubcf5\uc7a1\ud55c \ud2b9\uc131\uc744\n\ucd94\uac00\ud588\uc744 \ub54c \uac00\uc7a5 \ub208\uc5d0 \ub744\ub294 \ubd80\ubd84\uc740", 
          "start": 325.8
        }, 
        {
          "dur": 4.512, 
          "text": "\ud2b9\ud788 \ubaa8\ub378\uc758 \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c\n\ud2b9\uc131\uc774 \ud6e8\uc52c \ub354 \ubcf5\uc7a1\ud558\ub2e4\ub294 \uc810\uc774\uc5d0\uc694.", 
          "start": 333.728
        }, 
        {
          "dur": 5.149, 
          "text": "\ub274\ub7f0\uc774 \ud559\uc2b5\ud558\ub294 \uac83\uc740 \uc5ec\ub7ec\n\uae30\uc6b8\uae30\uc758 \uc120\uc774 \ub098\ud0c0\ub0b4\ub294 \uac83\ubcf4\ub2e4", 
          "start": 338.24
        }, 
        {
          "dur": 2.88, 
          "text": "\ud6e8\uc52c \ubcf5\uc7a1\ud574\uc694.", 
          "start": 343.389
        }, 
        {
          "dur": 7.471, 
          "text": "\ub354 \ubcf5\uc7a1\ud55c \ud2b9\uc131\uc744 \ucd94\uac00\ud558\uba74\n\ud6e8\uc52c \ube60\ub978 \uc18d\ub3c4\ub85c \uc774 \uace1\uc120\uc744", 
          "start": 346.269
        }, 
        {
          "dur": 1.41, 
          "text": "\ud559\uc2b5\ud558\uac8c \ub418\uc8e0.", 
          "start": 353.74
        }, 
        {
          "dur": 4.759, 
          "text": "\ud558\uc9c0\ub9cc \uc544\uc9c1\ub3c4 \uace1\uc120\uc774 \ub108\ubb34 \ubcf5\uc7a1\ud558\ub2c8\n\ub9dd\uc5d0 \ub108\ubb34 \ub9ce\uc740 \ub178\ub4dc\ub97c \uc0bd\uc785\ud558\ub294", 
          "start": 355.149
        }, 
        {
          "dur": 3.892, 
          "text": "\uccab \ubc88\uc9f8 \ud568\uc815\uc73c\ub85c \ub3cc\uc544\uac00\uac8c \ub3fc\uc694.", 
          "start": 359.908
        }, 
        {
          "dur": 5.41, 
          "text": "\uadf8\ub807\ub2e4\uba74 \uc77c\ubd80 \ub808\uc774\uc5b4\uc640\n\ub274\ub7f0\uc744 \uc81c\uac70\ud574 \ubcf4\uc8e0.", 
          "start": 363.8
        }, 
        {
          "dur": 6.509, 
          "text": "\ub808\uc774\uc5b4\uc640 \uac01 \ub808\uc774\uc5b4\uc5d0 \uc788\ub294 \ub274\ub7f0\uc758\n\uc218\ub97c \uc904\uc774\uace0 \ud559\uc2b5\ub960\ub3c4 \ub0ae\ucd94\uba74", 
          "start": 369.209
        }, 
        {
          "dur": 5.911, 
          "text": "\ub178\uc774\uc988\uac00 \uc2ec\ud55c \ub370\uc774\ud130\uac00 \uc788\ub354\ub77c\ub3c4\n\uc190\uc2e4 \uace1\uc120\uc774 \ud6e8\uc52c \ubd80\ub4dc\ub7ec\uc6cc\uc9c0\uace0", 
          "start": 375.718
        }, 
        {
          "dur": 6.62, 
          "text": "\ub370\uc774\ud130\uc758 \uc801\ud569\uc131\ub3c4 \ud06c\uac8c\n\ud5a5\uc0c1\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc8e0.", 
          "start": 381.629
        }, 
        {
          "dur": 5.73, 
          "text": "\ucd94\uac00\ud55c \ud2b9\uc131\uc774 \ub9ce\uc740 \ub3c4\uc6c0\uc774 \ub418\uc5c8\uace0\n\ubaa8\ub378\uc758 \ubcf5\uc7a1\ub3c4\ub97c \uc904\uc774\uba74\uc11c", 
          "start": 388.249
        }, 
        {
          "dur": 3.94, 
          "text": "\ubaa8\ub378\uc5d0 \ud3ec\ud568\ub41c \ub808\uc774\uc5b4 \uc218\ub97c \uc904\uc774\uace0 \ub098\ub2c8\n\ub370\uc774\ud130 \uacfc\uc801\ud569\uc744 \ud6e8\uc52c \ud6a8\uacfc\uc801\uc73c\ub85c", 
          "start": 393.978
        }, 
        {
          "dur": 1.291, 
          "text": "\ubc29\uc9c0\ud560 \uc218 \uc788\uac8c \ub418\uc5c8\uc5b4\uc694.", 
          "start": 397.918
        }, 
        {
          "dur": 5.699, 
          "text": "\ub2e8 \ud558\ub098\uc758 \ud788\ub4e0 \ub808\uc774\uc5b4\uc640 5\uac1c\uc758\n\ub274\ub7f0\ub9cc \uc788\ub294 \uc544\uc8fc \ub2e8\uc21c\ud55c \ubaa8\ub378\uc5d0\uc11c\ub3c4", 
          "start": 399.209
        }, 
        {
          "dur": 5.431, 
          "text": "\ud559\uc2b5\ub960\uacfc \uc815\uaddc\ud654\uc728, \ud65c\uc131\ud654 \ud568\uc218 \ub4f1\n\uba87 \uac00\uc9c0 \ub2e4\ub978 \ub9e4\uac1c\ubcc0\uc218\ub97c \ubcc0\uacbd\ud558\ub294 \uac83\uc73c\ub85c", 
          "start": 404.908
        }, 
        {
          "dur": 7.219, 
          "text": "\ubcf5\uc7a1\ud55c \ubaa8\ub378\uc5d0\uc11c\ubcf4\ub2e4 \ud6e8\uc52c \ub098\uc740\n\ud14c\uc2a4\ud2b8 \uc190\uc2e4\uacfc \ud6e8\uc52c \ubd80\ub4dc\ub7ec\uc6b4", 
          "start": 410.339
        }, 
        {
          "dur": 3.81, 
          "text": "\uc801\ud569\uc131 \uace1\uc120\uc744 \uc5bb\uc744 \uc218 \uc788\uc5b4\uc694.", 
          "start": 417.559
        }, 
        {
          "dur": 1.8, 
          "text": "\uc218\ub834 \uc18d\ub3c4\ub3c4 \ud6e8\uc52c \ube68\ub77c\uc9c0\uace0\uc694.", 
          "start": 421.369
        }
      ]
    }, 
    {
      "lang": "es-419", 
      "captions": [
        {
          "dur": 3.079, 
          "text": "Este es el ejercicio\nde la espiral de red neural de MLCC.", 
          "start": 0.55
        }, 
        {
          "dur": 6.72, 
          "text": "Si usamos los par\u00e1metros predeterminados\npara el modelo,", 
          "start": 3.629
        }, 
        {
          "dur": 2.811, 
          "text": "no funcionar\u00e1 bien.", 
          "start": 10.349
        }, 
        {
          "dur": 7.679, 
          "text": "Incluso despu\u00e9s de 2,500 iteraciones,\nla p\u00e9rdida de la prueba es mayor al 50%,", 
          "start": 13.16
        }, 
        {
          "dur": 2.84, 
          "text": "y el modelo no aprende de los datos.", 
          "start": 20.839
        }, 
        {
          "dur": 3.821, 
          "text": "Intentemos mejorarlo.", 
          "start": 23.679
        }, 
        {
          "dur": 5.168, 
          "text": "Si bien los datos son complejos\nen estas espirales intercalados,", 
          "start": 27.5
        }, 
        {
          "dur": 4.381, 
          "text": "no se necesita un modelo profundo ni supercomplejo\npara resolver el problema si tenemos las funciones", 
          "start": 32.668
        }, 
        {
          "dur": 1.0, 
          "text": "y los par\u00e1metros indicados.", 
          "start": 37.049
        }, 
        {
          "dur": 4.61, 
          "text": "Hablar\u00e9 sobre algunos obst\u00e1culos\npara tenerlos en cuenta cuando resolvemos", 
          "start": 38.049
        }, 
        {
          "dur": 2.579, 
          "text": "problemas como estos.", 
          "start": 42.659
        }, 
        {
          "dur": 6.94, 
          "text": "En la primera tarea, debemos entrenar el modelo\nsolo con X1 y X2,", 
          "start": 45.238
        }, 
        {
          "dur": 1.77, 
          "text": "sin ingenier\u00eda de funciones.", 
          "start": 52.179
        }, 
        {
          "dur": 8.5, 
          "text": "En el primer modelo, agregar\u00e9 muchas capas\ny muchas neuronas a cada capa.", 
          "start": 53.948
        }, 
        {
          "dur": 6.211, 
          "text": "M\u00e1s es mejor, \u00bfno?", 
          "start": 62.448
        }, 
        {
          "dur": 8.41, 
          "text": "Este es el primer problema,\ndemasiados nodos en el gr\u00e1fico.", 
          "start": 68.659
        }, 
        {
          "dur": 4.61, 
          "text": "El modelo es lento, usa muchos recursos\npara ajustar las ponderaciones", 
          "start": 77.069
        }, 
        {
          "dur": 1.83, 
          "text": "en el gr\u00e1fico cuando hay tantos nodos.", 
          "start": 81.68
        }, 
        {
          "dur": 3.599, 
          "text": "Cada iteraci\u00f3n lleva mucho tiempo.", 
          "start": 83.51
        }, 
        {
          "dur": 5.61, 
          "text": "Adem\u00e1s, no se puede interpretar bien,\ncon tantas capas y nodos en el gr\u00e1fico.", 
          "start": 87.109
        }, 
        {
          "dur": 4.44, 
          "text": "La primera capa del modelo tiene algo de sentido.", 
          "start": 92.719
        }, 
        {
          "dur": 3.39, 
          "text": "Son estas funciones lineales en diferentes \u00e1ngulos.", 
          "start": 97.159
        }, 
        {
          "dur": 4.541, 
          "text": "Al avanzar hacia el medio del gr\u00e1fico,\nno queda claro", 
          "start": 100.549
        }, 
        {
          "dur": 6.239, 
          "text": "c\u00f3mo contribuyen las funciones al resultado\nde datos de la espiral.", 
          "start": 105.09
        }, 
        {
          "dur": 6.71, 
          "text": "Se puede ver c\u00f3mo mejor\u00f3 la p\u00e9rdida de prueba.", 
          "start": 111.329
        }, 
        {
          "dur": 6.83, 
          "text": "Est\u00e1 por debajo del 50%,\npero todav\u00eda no es una curva suave.", 
          "start": 118.04
        }, 
        {
          "dur": 4.87, 
          "text": "Reduzcamos los nodos del modelo\npara que sea m\u00e1s r\u00e1pido y tenga m\u00e1s sentido", 
          "start": 124.87
        }, 
        {
          "dur": 7.9, 
          "text": "y ajustemos los par\u00e1metros\npara mejorar la p\u00e9rdida de prueba.", 
          "start": 129.74
        }, 
        {
          "dur": 5.56, 
          "text": "Ahora, tenemos cuatro capas ocultas en el gr\u00e1fico\ncon 20 nodos en total.", 
          "start": 137.639
        }, 
        {
          "dur": 2.66, 
          "text": "Menos de la mitad que antes.", 
          "start": 143.199
        }, 
        {
          "dur": 2.42, 
          "text": "No es una red muy simple, pero es mejor.", 
          "start": 145.86
        }, 
        {
          "dur": 2.28, 
          "text": "Se entrenar\u00e1 m\u00e1s r\u00e1pido.", 
          "start": 148.28
        }, 
        {
          "dur": 5.53, 
          "text": "Tambi\u00e9n reducir\u00e9 la tasa de aprendizaje\npara eliminar los saltos", 
          "start": 150.56
        }, 
        {
          "dur": 2.03, 
          "text": "en la curva de p\u00e9rdida.", 
          "start": 156.09
        }, 
        {
          "dur": 5.83, 
          "text": "Con suerte, la curva de p\u00e9rdida ser\u00e1 m\u00e1s uniforme\ny los datos de prueba no se sobreajustar\u00e1n.", 
          "start": 158.12
        }, 
        {
          "dur": 4.31, 
          "text": "Veamos cu\u00e1l es el resultado.", 
          "start": 163.949
        }, 
        {
          "dur": 7.581, 
          "text": "Con menos nodos\ny una tasa de aprendizaje m\u00e1s lenta,", 
          "start": 168.259
        }, 
        {
          "dur": 1.0, 
          "text": "el modelo aprende m\u00e1s r\u00e1pido.", 
          "start": 175.84
        }, 
        {
          "dur": 5.289, 
          "text": "Las iteraciones son m\u00e1s r\u00e1pidas,\npero la p\u00e9rdida de prueba se mantiene.", 
          "start": 176.84
        }, 
        {
          "dur": 5.021, 
          "text": "Vemos que las curvas de p\u00e9rdida\ntodav\u00eda oscilan y que el resultado del modelo", 
          "start": 182.129
        }, 
        {
          "dur": 3.729, 
          "text": "no es tan uniforme como las espirales.", 
          "start": 187.15
        }, 
        {
          "dur": 4.771, 
          "text": "Esto nos lleva al pr\u00f3ximo problema: la no regularizaci\u00f3n.", 
          "start": 190.879
        }, 
        {
          "dur": 4.009, 
          "text": "La regularizaci\u00f3n penaliza los modelos complejos\ny el sobreajuste.", 
          "start": 195.65
        }, 
        {
          "dur": 5.47, 
          "text": "Si agregamos esto,\nlogramos una curva de p\u00e9rdida m\u00e1s suave", 
          "start": 199.659
        }, 
        {
          "dur": 6.14, 
          "text": "y un modelo mejor que no se sobreajusta\na los datos de entrenamiento.", 
          "start": 205.129
        }, 
        {
          "dur": 7.41, 
          "text": "Agreguemos regularizaci\u00f3n al modelo\npara ver si mejora la curva y el ajuste.", 
          "start": 211.27
        }, 
        {
          "dur": 5.779, 
          "text": "La regularizaci\u00f3n no debe ser demasiado alta,\nde lo contrario no aprender\u00e1 ninguna complejidad", 
          "start": 218.68
        }, 
        {
          "dur": 2.241, 
          "text": "en el modelo.", 
          "start": 224.459
        }, 
        {
          "dur": 9.98, 
          "text": "Al agregar la regularizaci\u00f3n,\nse logra una curva de p\u00e9rdida m\u00e1s suave", 
          "start": 226.699
        }, 
        {
          "dur": 7.75, 
          "text": "y un resultado del modelo con menos picos.", 
          "start": 236.68
        }, 
        {
          "dur": 2.93, 
          "text": "Existen muchos otros par\u00e1metros\nque puedes probar en el modelo.", 
          "start": 244.43
        }, 
        {
          "dur": 3.849, 
          "text": "Puedes cambiar el tama\u00f1o del lote;\nsi es muy peque\u00f1o, entrenar el modelo", 
          "start": 247.36
        }, 
        {
          "dur": 2.25, 
          "text": "tomar\u00e1 mucho tiempo.", 
          "start": 251.209
        }, 
        {
          "dur": 2.67, 
          "text": "Si es muy grande,\nhabr\u00e1 problemas con la convergencia.", 
          "start": 253.459
        }, 
        {
          "dur": 5.21, 
          "text": "si la tasa de aprendizaje es muy alta,\nsaltar\u00e1 demasiado y tambi\u00e9n habr\u00e1 problemas", 
          "start": 256.129
        }, 
        {
          "dur": 1.0, 
          "text": "con la convergencia.", 
          "start": 261.339
        }, 
        {
          "dur": 4.859, 
          "text": "Tambi\u00e9n puedes cambiar la tasa de regularizaci\u00f3n.", 
          "start": 262.339
        }, 
        {
          "dur": 3.951, 
          "text": "Este ser\u00eda un muy buen modelo\nsi los datos de entrenamiento fuesen menos ruidosos.", 
          "start": 267.199
        }, 
        {
          "dur": 2.911, 
          "text": "El ruido llega a 80 aqu\u00ed.", 
          "start": 271.149
        }, 
        {
          "dur": 5.81, 
          "text": "Si la bajamos a 30\ny entrenamos el modelo,", 
          "start": 274.06
        }, 
        {
          "dur": 2.06, 
          "text": "podemos lograr una p\u00e9rdida bastante buena.", 
          "start": 279.87
        }, 
        {
          "dur": 4.238, 
          "text": "Pero con el ruido alto,\nes mucho m\u00e1s eficaz", 
          "start": 281.93
        }, 
        {
          "dur": 5.71, 
          "text": "agregar la ingenier\u00eda de funciones\nque sumar m\u00e1s capas o ajustar los par\u00e1metros.", 
          "start": 286.168
        }, 
        {
          "dur": 8.62, 
          "text": "Al reducir el ruido,\nse logra una p\u00e9rdida de prueba mucho mejor.", 
          "start": 291.879
        }, 
        {
          "dur": 3.711, 
          "text": "Incluso si solo usamos funciones lineales.\nPero, si tenemos datos ruidosos,", 
          "start": 300.499
        }, 
        {
          "dur": 6.049, 
          "text": "necesitaremos la ingenier\u00eda de funciones\npara mejorar la p\u00e9rdida de prueba.", 
          "start": 304.209
        }, 
        {
          "dur": 2.49, 
          "text": "Entonces, el ruido vuelve a 80%.", 
          "start": 310.259
        }, 
        {
          "dur": 2.361, 
          "text": "Agreguemos algunas funciones complejas.", 
          "start": 312.749
        }, 
        {
          "dur": 6.358, 
          "text": "Agreguemos seno, X1 al cuadrado\ny X2 al cuadrado.", 
          "start": 315.11
        }, 
        {
          "dur": 4.332, 
          "text": "Veamos el resultado del modelo.", 
          "start": 321.468
        }, 
        {
          "dur": 7.928, 
          "text": "Al agregar funciones m\u00e1s complejas,\nnotamos que estas funciones", 
          "start": 325.8
        }, 
        {
          "dur": 4.512, 
          "text": "aparecen especialmente\nen la primera capa del modelo.", 
          "start": 333.728
        }, 
        {
          "dur": 5.149, 
          "text": "La complejidad en el aprendizaje de las neuronas\nno se limita a unas pocas l\u00edneas", 
          "start": 338.24
        }, 
        {
          "dur": 2.88, 
          "text": "con diferentes inclinaciones.", 
          "start": 343.389
        }, 
        {
          "dur": 7.471, 
          "text": "Entonces, al agregar funciones m\u00e1s complejas,\nlas curvas se aprenden", 
          "start": 346.269
        }, 
        {
          "dur": 1.41, 
          "text": "a una tasa mucho m\u00e1s r\u00e1pida.", 
          "start": 353.74
        }, 
        {
          "dur": 4.759, 
          "text": "Pero la curva sigue siendo muy compleja,\nlo que nos recuerda el primer problema:", 
          "start": 355.149
        }, 
        {
          "dur": 3.892, 
          "text": "hay demasiados nodos en la red.", 
          "start": 359.908
        }, 
        {
          "dur": 5.41, 
          "text": "Eliminemos algunas de estas capas y algunas neuronas.", 
          "start": 363.8
        }, 
        {
          "dur": 6.509, 
          "text": "Al reducir la cantidad de capas\ny de neuronas en cada capa,", 
          "start": 369.209
        }, 
        {
          "dur": 5.911, 
          "text": "tambi\u00e9n la tasa de aprendizaje,\nnos da una curva de aprendizaje m\u00e1s suave", 
          "start": 375.718
        }, 
        {
          "dur": 6.62, 
          "text": "que se ajusta mucho mejor a los datos,\nincluso si los datos son ruidosos.", 
          "start": 381.629
        }, 
        {
          "dur": 5.73, 
          "text": "Entonces, las funciones que agregamos\nson de gran ayuda. Adem\u00e1s, al reducir", 
          "start": 388.249
        }, 
        {
          "dur": 3.94, 
          "text": "la complejidad del modelo\ny la cantidad de capas,", 
          "start": 393.978
        }, 
        {
          "dur": 1.291, 
          "text": "logramos no sobreajustar los datos.", 
          "start": 397.918
        }, 
        {
          "dur": 5.699, 
          "text": "Incluso con un modelo muy muy simple\nde una sola capa oculta y cinco neuronas,", 
          "start": 399.209
        }, 
        {
          "dur": 5.431, 
          "text": "al cambiar un par de par\u00e1metros,\ncomo la tasa de aprendizaje, la de regularizaci\u00f3n", 
          "start": 404.908
        }, 
        {
          "dur": 7.219, 
          "text": "y la funci\u00f3n de activaci\u00f3n,\nse logra una mejor p\u00e9rdida de prueba", 
          "start": 410.339
        }, 
        {
          "dur": 3.81, 
          "text": "y una curva de ajuste m\u00e1s uniforme\nque en los modelos m\u00e1s complejos.", 
          "start": 417.559
        }, 
        {
          "dur": 1.8, 
          "text": "Adem\u00e1s, la convergencia es mucho m\u00e1s r\u00e1pida.", 
          "start": 421.369
        }
      ]
    }
  ]
}